{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, date_format, weekofyear, month, quarter, year\n",
    "import os\n",
    "from pyspark.sql.types import *\n",
    "import requests\n",
    "import psutil\n",
    "from datetime import datetime, timedelta\n",
    "import glob\n",
    "import time\n",
    "from pyspark.sql.functions import col, count, when\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession, functions as F, Window\n",
    "from pyspark.sql.functions import broadcast\n",
    "import shutil\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from google.cloud import bigquery\n",
    "import pyarrow.parquet as pq\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions, GoogleCloudOptions, SetupOptions, StandardOptions\n",
    "from google.cloud import storage\n",
    "import tempfile\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cores = psutil.cpu_count(logical=False)  \n",
    "# --> Counts the number of physical CPU cores / Conta o número de núcleos físicos de CPU <--\n",
    "\n",
    "NUM_PARTITIONS = max(num_cores * 2, 16)  \n",
    "# --> Sets the number of partitions for the Spark job, based on the number of cores, with a minimum of 16 partitions / Define o número de partições para o trabalho do Spark, baseado no número de núcleos, com um mínimo de 16 partições <--\n",
    "\n",
    "print(f\"Número de partições ajustado para: {NUM_PARTITIONS}\")  \n",
    "num_cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INICIAR SESSÃO SPARK COM OTIMIZAÇÕES PARA O MAC M2 (8GB RAM)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Corrigir_Parquet\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", str(NUM_PARTITIONS)) \\\n",
    "    .config(\"spark.default.parallelism\", \"6\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.80\") \\\n",
    "    .config(\"spark.memory.storageFraction\", \"0.5\") \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"128MB\") \\\n",
    "    .config(\"spark.cleaner.referenceTracking.cleanCheckpoints\", \"false\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"60000ms\") \\\n",
    "    .config(\"spark.task.cpus\", \"1\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# REDUZIR LOGS PARA EVITAR POLUIÇÃO NO CONSOLE\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(\"\\n SparkSession configurada com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_BLOCKS = \"/Users/rodrigocampos/Library/Mobile Documents/com~apple~CloudDocs/processed_data/blockchain_blocks_part\"\n",
    "PATH_TRANSACTIONS = \"/Users/rodrigocampos/Library/Mobile Documents/com~apple~CloudDocs/processed_data/blockchain_transactions_part\"\n",
    "PATH_ADDRESSES = \"/Users/rodrigocampos/Library/Mobile Documents/com~apple~CloudDocs/processed_data/blockchain_addresses_part\"\n",
    "SAVE_PATH = \"/Users/rodrigocampos/Library/Mobile Documents/com~apple~CloudDocs/bitcoin_features/features_temp/df_bitcoin_features.parquet\"\n",
    "METADATA_PATH = \"/Users/rodrigocampos/Library/Mobile Documents/com~apple~CloudDocs/processed_data/metadata.txt\"\n",
    "USD_PATH = \"/Users/rodrigocampos/Library/Mobile Documents/com~apple~CloudDocs/bitcoin_features/features_dolar_parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blocks = spark.read.parquet(PATH_BLOCKS)\n",
    "df_transactions = spark.read.parquet(PATH_TRANSACTIONS)\n",
    "df_addresses = spark.read.parquet(PATH_ADDRESSES)\n",
    "# df_blocks_features = spark.read.parquet(SAVE_PATH)\n",
    "# df_price_dolar = spark.read.parquet(USD_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "menor_bloco = df_blocks.select(\"block_height\").orderBy(\"block_height\").first()[0]  \n",
    "# --> Gets the minimum block height by ordering the 'block_height' column in ascending order / Obtém a altura mínima do bloco ordenando a coluna 'block_height' em ordem crescente <--\n",
    "\n",
    "maior_bloco = df_blocks.select(\"block_height\").orderBy(F.desc(\"block_height\")).first()[0]  \n",
    "# --> Gets the maximum block height by ordering the 'block_height' column in descending order / Obtém a altura máxima do bloco ordenando a coluna 'block_height' em ordem decrescente <--\n",
    "\n",
    "print(f\"Menor bloco salvo: {menor_bloco}\")  \n",
    "# --> Prints the minimum block height / Imprime a altura mínima do bloco <--\n",
    "\n",
    "print(f\"Maior bloco salvo: {maior_bloco}\")  \n",
    "# --> Prints the maximum block height / Imprime a altura máxima do bloco <--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blocks.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blocks_clean = df_blocks.dropDuplicates()  \n",
    "# --> Removes duplicate rows from 'df_blocks' DataFrame / Remove as linhas duplicadas do DataFrame 'df_blocks' <--\n",
    "\n",
    "df_transactions_clean = df_transactions.dropDuplicates()  \n",
    "# --> Removes duplicate rows from 'df_transactions' DataFrame / Remove as linhas duplicadas do DataFrame 'df_transactions' <--\n",
    "\n",
    "df_addresses_clean = df_addresses.dropDuplicates()  \n",
    "# --> Removes duplicate rows from 'df_addresses' DataFrame / Remove as linhas duplicadas do DataFrame 'df_addresses' <--\n",
    "\n",
    "num_duplicates_blocks = df_blocks_clean.groupBy(df_blocks_clean.columns).count().filter(F.col(\"count\") > 1).count()  \n",
    "# --> Counts the number of duplicate rows in the 'df_blocks_clean' DataFrame / Conta o número de linhas duplicadas no DataFrame 'df_blocks_clean' <--\n",
    "\n",
    "num_duplicates_transactions = df_transactions_clean.groupBy(df_transactions_clean.columns).count().filter(F.col(\"count\") > 1).count()  \n",
    "# --> Counts the number of duplicate rows in the 'df_transactions_clean' DataFrame / Conta o número de linhas duplicadas no DataFrame 'df_transactions_clean' <--\n",
    "\n",
    "num_duplicates_addresses = df_addresses_clean.groupBy(df_addresses_clean.columns).count().filter(F.col(\"count\") > 1).count()  \n",
    "# --> Counts the number of duplicate rows in the 'df_addresses_clean' DataFrame / Conta o número de linhas duplicadas no DataFrame 'df_addresses_clean' <--\n",
    "\n",
    "print(f\"Blocos duplicados: {num_duplicates_blocks}\")  \n",
    "# --> Prints the number of duplicate blocks / Imprime o número de blocos duplicados <--\n",
    "\n",
    "print(f\"Transações duplicadas: {num_duplicates_transactions}\")  \n",
    "# --> Prints the number of duplicate transactions / Imprime o número de transações duplicadas <--\n",
    "\n",
    "print(f\"Endereços duplicados: {num_duplicates_addresses}\")  \n",
    "# --> Prints the number of duplicate addresses / Imprime o número de endereços duplicados <--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAST_UPDATE_FILE = \"/Users/rodrigocampos/Library/Mobile Documents/com~apple~CloudDocs/bitcoin_features/last_update.txt\"  \n",
    "# --> Path to the file where the last update timestamp will be saved / Caminho para o arquivo onde a data do último update será salva <--\n",
    "\n",
    "df_blocks = spark.read.parquet(PATH_BLOCKS)  \n",
    "# --> Reads the 'blockchain_blocks' Parquet file into a Spark DataFrame / Lê o arquivo Parquet 'blockchain_blocks' para um DataFrame do Spark <--\n",
    "\n",
    "min_date = df_blocks.agg(F.min(\"block_timestamp\")).collect()[0][0]  \n",
    "# --> Gets the minimum timestamp (oldest date) from the 'block_timestamp' column / Obtém o timestamp mínimo (data mais antiga) da coluna 'block_timestamp' <--\n",
    "\n",
    "if min_date:  \n",
    "    # --> If a valid minimum date is found / Se uma data mínima válida for encontrada <--\n",
    "    min_date_str = min_date.strftime(\"%Y-%m-%d %H:%M:%S\")  \n",
    "    # --> Converts the date to a string format / Converte a data para o formato de string <--\n",
    "\n",
    "    with open(LAST_UPDATE_FILE, \"w\") as f:  \n",
    "        # --> Opens the file to write the date string / Abre o arquivo para escrever a string da data <--\n",
    "        f.write(min_date_str)  \n",
    "\n",
    "    print(f\"Data mais antiga ({min_date_str}) salva em {LAST_UPDATE_FILE}\")  \n",
    "    # --> Prints the saved oldest date to the console / Imprime a data mais antiga salva no console <--\n",
    "else:  \n",
    "    print(\"Erro: Nenhuma data encontrada no DataFrame.\")  \n",
    "    # --> Prints an error message if no date was found / Imprime uma mensagem de erro se nenhuma data for encontrada <--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.utcnow()  \n",
    "# --> Gets the current UTC time / Obtém o horário atual em UTC <--\n",
    "\n",
    "three_days_ago = now - timedelta(days=3)  \n",
    "# --> Calculates the timestamp for three days ago / Calcula o timestamp de três dias atrás <--\n",
    "\n",
    "COINGECKO_API_URL = \"https://api.coingecko.com/api/v3/coins/bitcoin/market_chart/range\"  \n",
    "# --> Base URL for the CoinGecko API to fetch historical market data / URL base da API CoinGecko para buscar dados históricos de mercado <--\n",
    "\n",
    "PARAMS = {  \n",
    "    \"vs_currency\": \"usd\",  \n",
    "    # --> Sets the currency for the price data to USD / Define a moeda para os dados de preço como USD <--\n",
    "    \"from\": int(three_days_ago.timestamp()),  \n",
    "    # --> Sets the starting timestamp for the query (3 days ago) / Define o timestamp de início da consulta (3 dias atrás) <--\n",
    "    \"to\": int(now.timestamp())  \n",
    "    # --> Sets the end timestamp for the query (current time) / Define o timestamp final da consulta (hora atual) <--\n",
    "}\n",
    "\n",
    "response = requests.get(COINGECKO_API_URL, params=PARAMS)  \n",
    "# --> Sends a GET request to the CoinGecko API to fetch market chart data / Envia uma solicitação GET para a API CoinGecko para buscar dados do gráfico de mercado <--\n",
    "\n",
    "data = response.json()  \n",
    "# --> Parses the JSON response from the API / Analisa a resposta JSON da API <--\n",
    "\n",
    "df = pd.DataFrame(data[\"prices\"], columns=[\"timestamp\", \"price\"])  \n",
    "# --> Converts the data into a DataFrame with columns for timestamp and price / Converte os dados em um DataFrame com as colunas de timestamp e preço <--\n",
    "\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], unit=\"ms\")  \n",
    "# --> Converts the timestamp from milliseconds to a readable datetime format / Converte o timestamp de milissegundos para um formato de data e hora legível <--\n",
    "\n",
    "df[\"time_diff\"] = df[\"timestamp\"].diff()  \n",
    "# --> Calculates the difference between consecutive timestamps / Calcula a diferença entre timestamps consecutivos <--\n",
    "\n",
    "print(df.head(10))  \n",
    "# --> Prints the first 10 rows of the DataFrame / Imprime as primeiras 10 linhas do DataFrame <--\n",
    "\n",
    "print(df[\"time_diff\"].value_counts())  \n",
    "# --> Prints the count of each unique time difference / Imprime a contagem de cada diferença de tempo única <--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"time_diff_minutes\"] = df[\"time_diff\"].dt.total_seconds() / 60  \n",
    "# --> Converts the time difference from seconds to minutes / Converte a diferença de tempo de segundos para minutos <--\n",
    "\n",
    "plt.hist(df[\"time_diff_minutes\"], bins=20, edgecolor=\"black\")  \n",
    "# --> Creates a histogram to visualize the distribution of time differences / Cria um histograma para visualizar a distribuição das diferenças de tempo <--\n",
    "\n",
    "plt.xlabel(\"Intervalo entre atualizações (minutos)\")  \n",
    "# --> Labels the x-axis with the time difference in minutes / Rotula o eixo x com a diferença de tempo em minutos <--\n",
    "\n",
    "plt.ylabel(\"Frequência\")  \n",
    "# --> Labels the y-axis with the frequency of each time interval / Rotula o eixo y com a frequência de cada intervalo de tempo <--\n",
    "\n",
    "plt.title(\"Distribuição da frequência de atualização do preço do Bitcoin\")  \n",
    "# --> Adds a title to the histogram / Adiciona um título ao histograma <--\n",
    "\n",
    "plt.show()  \n",
    "# --> Displays the histogram / Exibe o histograma <--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import glob\n",
    "\n",
    "\n",
    "PARQUET_PATH = \"/Users/rodrigocampos/Library/Mobile Documents/com~apple~CloudDocs/bitcoin_features/features_dolar_parquet\"\n",
    "LAST_UPDATE_FILE = \"/Users/rodrigocampos/Library/Mobile Documents/com~apple~CloudDocs/bitcoin_features/last_update.txt\"\n",
    "\n",
    "def get_last_saved_date():\n",
    "    if os.path.exists(LAST_UPDATE_FILE):\n",
    "        with open(LAST_UPDATE_FILE, \"r\") as f:\n",
    "            date_str = f.read().strip()\n",
    "            return pd.to_datetime(date_str) if date_str else None\n",
    "    return None\n",
    "    # --> Reads the last saved date from a file / Lê a última data salva de um arquivo <--\n",
    "\n",
    "def update_last_saved_date(date):\n",
    "    with open(LAST_UPDATE_FILE, \"w\") as f:\n",
    "        f.write(date.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    # --> Updates the last saved date to the file / Atualiza a última data salva no arquivo <--\n",
    "\n",
    "last_saved_date = get_last_saved_date()\n",
    "print(f\"Última data salva: {last_saved_date}\")\n",
    "# --> Prints the last saved date / Imprime a última data salva <--\n",
    "\n",
    "if last_saved_date is None:\n",
    "    last_saved_date = datetime.utcnow() - timedelta(days=30)\n",
    "    # --> If no date is found, sets the default to 30 days ago / Se nenhuma data for encontrada, define a data padrão como 30 dias atrás <--\n",
    "\n",
    "now = datetime.utcnow()\n",
    "to_timestamp = int(now.timestamp())\n",
    "# --> Converts the current UTC time to a timestamp / Converte o horário UTC atual para um timestamp <--\n",
    "\n",
    "CRYPTOCOMPARE_API = \"https://min-api.cryptocompare.com/data/v2/histohour\"\n",
    "SYMBOL = \"BTC\"\n",
    "CURRENCY = \"USD\"\n",
    "LIMIT = 2000  \n",
    "\n",
    "all_data = []\n",
    "current_to_ts = to_timestamp\n",
    "\n",
    "while True:\n",
    "    response = requests.get(CRYPTOCOMPARE_API, params={\n",
    "        \"fsym\": SYMBOL,\n",
    "        \"tsym\": CURRENCY,\n",
    "        \"limit\": LIMIT,\n",
    "        \"toTs\": current_to_ts\n",
    "    })\n",
    "    # --> Makes an API request to CryptoCompare for hourly Bitcoin data / Faz uma requisição à API CryptoCompare para dados horários do Bitcoin <--\n",
    "\n",
    "    json_data = response.json()\n",
    "    \n",
    "    if json_data[\"Response\"] != \"Success\":\n",
    "        print(\"Erro na resposta da API:\", json_data)\n",
    "        break\n",
    "    # --> Checks if the API response is successful / Verifica se a resposta da API foi bem-sucedida <--\n",
    "\n",
    "    df_batch = pd.DataFrame(json_data[\"Data\"][\"Data\"])\n",
    "    df_batch[\"datetime\"] = pd.to_datetime(df_batch[\"time\"], unit=\"s\")\n",
    "    df_batch = df_batch[[\"datetime\", \"close\"]]  \n",
    "    df_batch.columns = [\"date\", \"price\"]\n",
    "    # --> Converts the API response data into a pandas DataFrame / Converte os dados da resposta da API para um DataFrame pandas <--\n",
    "\n",
    "    if df_batch[\"date\"].min() <= last_saved_date:\n",
    "        df_batch = df_batch[df_batch[\"date\"] > last_saved_date]\n",
    "        all_data.append(df_batch)\n",
    "        break\n",
    "    # --> If new data is available, appends it to the list / Se dados novos estão disponíveis, adiciona-os à lista <--\n",
    "\n",
    "    all_data.append(df_batch)\n",
    "    current_to_ts = int(df_batch[\"date\"].min().timestamp()) - 1\n",
    "    # --> Updates the current timestamp to avoid duplicate data / Atualiza o timestamp atual para evitar dados duplicados <--\n",
    "\n",
    "if not all_data:\n",
    "    print(\"Nenhum dado novo encontrado.\")\n",
    "    exit()\n",
    "# --> Exits if no new data is found / Sai se nenhum dado novo for encontrado <--\n",
    "\n",
    "df_new = pd.concat(all_data, ignore_index=True).drop_duplicates(\"date\")\n",
    "df_new = df_new[df_new[\"date\"] > last_saved_date]\n",
    "# --> Concatenates all the batches and filters out the already saved data / Concatena todos os lotes e filtra os dados já salvos <--\n",
    "\n",
    "df_new[\"year\"] = df_new[\"date\"].dt.year\n",
    "df_new[\"month\"] = df_new[\"date\"].dt.month\n",
    "# --> Adds year and month columns for partitioning / Adiciona colunas de ano e mês para particionamento <--\n",
    "\n",
    "df_new_spark = spark.createDataFrame(df_new)\n",
    "# --> Converts the pandas DataFrame to a Spark DataFrame / Converte o DataFrame pandas para um DataFrame Spark <--\n",
    "\n",
    "parquet_files = glob.glob(os.path.join(PARQUET_PATH, \"**/*.parquet\"), recursive=True)\n",
    "\n",
    "if parquet_files:\n",
    "    try:\n",
    "        df_old_spark = spark.read.parquet(PARQUET_PATH)\n",
    "        print(f\"Dados antigos carregados: {df_old_spark.count()} registros\")\n",
    "        # --> Reads the existing Parquet data / Lê os dados Parquet existentes <--\n",
    "\n",
    "        df_price_dolar = df_new_spark.join(df_old_spark, \"date\", \"left_anti\").dropDuplicates([\"date\"])\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar dados antigos: {e}\")\n",
    "        df_price_dolar = df_new_spark\n",
    "else:\n",
    "    print(\"Nenhum arquivo Parquet encontrado. Iniciando novo dataset.\")\n",
    "    df_price_dolar = df_new_spark\n",
    "# --> Joins the new data with existing data, excluding duplicates / Junta os dados novos com os dados existentes, excluindo duplicados <--\n",
    "\n",
    "if df_price_dolar.count() > 0:\n",
    "    df_price_dolar.write.mode(\"append\").partitionBy(\"year\", \"month\").parquet(PARQUET_PATH)\n",
    "    # --> Writes the new data to Parquet, partitioned by year and month / Grava os dados novos no Parquet, particionados por ano e mês <--\n",
    "    \n",
    "    latest_date = df_price_dolar.toPandas()[\"date\"].max()\n",
    "    update_last_saved_date(latest_date)\n",
    "    # --> Updates the last saved date after successful data save / Atualiza a última data salva após a gravação bem-sucedida dos dados <--\n",
    "    \n",
    "    print(f\"Nova última data salva: {latest_date}\")\n",
    "    print(\"Novo dado adicionado ao Parquet!\")\n",
    "else:\n",
    "    print(\"Nenhum novo dado diferente para salvar.\")\n",
    "# --> Prints a message if no new data is available to save / Imprime uma mensagem se não houver dados novos para salvar <--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_price_dolar = spark.read.parquet(USD_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_price_dolar = spark.read.parquet(USD_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_price_dolar.select(\n",
    "    F.min(\"date\").alias(\"Min date\"),\n",
    "    F.max(\"date\").alias(\"Max date\")\n",
    ").show(truncate=False)\n",
    "# --> Seleciona a data mínima e máxima do DataFrame / Selects the minimum and maximum date from the DataFrame <--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_price_dolar.select(\"date\").distinct().count()\n",
    "# --> Conta o número de datas distintas no DataFrame / Counts the number of distinct dates in the DataFrame <--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_price_dolar.printSchema()\n",
    "df_price_dolar.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_price_dolar.select([count(when(col(c).isNull(), c)).alias(c) for c in df_price_dolar.columns]).show()\n",
    "# --> Conta o número de valores nulos em cada coluna do DataFrame / Counts the number of null values in each column of the DataFrame <--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_price_dolar.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_price_dolar.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blocks_minute = df_blocks.withColumn(\"minute\", F.date_format(\"block_timestamp\", \"yyyy-MM-dd HH:mm\"))\n",
    "# --> Cria uma nova coluna \"minute\" com o formato de data e hora até o minuto / Creates a new column \"minute\" with the date and time formatted up to the minute <--\n",
    "\n",
    "df_range = df_blocks_minute.agg(\n",
    "    F.min(\"minute\").alias(\"start\"),\n",
    "    F.max(\"minute\").alias(\"end\")\n",
    ").first()\n",
    "# --> Calcula o intervalo de tempo (start e end) com base na coluna \"minute\" / Calculates the time range (start and end) based on the \"minute\" column <--\n",
    "\n",
    "start_time, end_time = df_range[\"start\"], df_range[\"end\"]\n",
    "# --> Extrai as variáveis de tempo inicial e final / Extracts the start and end time variables <--\n",
    "\n",
    "df_time = spark.sql(f\"\"\"\n",
    "  SELECT explode(sequence(to_timestamp('{start_time}'), to_timestamp('{end_time}'), interval 1 minute)) as minute\n",
    "\"\"\")\n",
    "# --> Cria uma sequência de minutos entre o tempo inicial e final / Creates a sequence of minutes between the start and end time <--\n",
    "\n",
    "df_blocks_filled = df_time.join(df_blocks_minute, \"minute\", \"left\").fillna(0)\n",
    "# --> Faz o join entre a sequência de minutos e os dados de blocos, preenchendo com 0 onde há dados faltantes / Joins the minute sequence with block data, filling with 0 where data is missing <--\n",
    "\n",
    "window_spec = Window.orderBy(\"minute\")\n",
    "# --> Define a janela para ordenação por minuto / Defines the window to order by minute <--\n",
    "\n",
    "df_blocks_features = df_blocks_filled.withColumns({\n",
    "    \"btc_moved_rolling_mean\": F.avg(\"total_btc_moved\").over(window_spec.rowsBetween(-5, 0)),\n",
    "    \"btc_moved_lag_1\": F.lag(\"total_btc_moved\").over(window_spec),\n",
    "    \"day\": F.date_format(F.col(\"block_timestamp\"), \"yyyy-MM-dd\"),\n",
    "    \"week\": F.weekofyear(F.col(\"block_timestamp\")),\n",
    "    \"month\": F.month(F.col(\"block_timestamp\")),\n",
    "    \"semester\": F.quarter(F.col(\"block_timestamp\")),\n",
    "    \"year\": F.year(F.col(\"block_timestamp\"))\n",
    "})\n",
    "# --> Cria novas colunas com médias móveis, lags, e informações sobre dia, semana, mês, semestre e ano / Creates new columns with rolling means, lags, and information about day, week, month, semester, and year <--\n",
    "\n",
    "df_blocks_features = df_blocks_features.filter(F.col(\"block_height\").isNotNull() & (F.col(\"block_height\") > 0))\n",
    "# --> Filtra registros com \"block_height\" não nulo e maior que 0 / Filters records with non-null \"block_height\" and greater than 0 <--\n",
    "\n",
    "agg_blocks = df_blocks.agg(\n",
    "    F.min(\"block_height\").alias(\"menor_bloco\"),\n",
    "    F.max(\"block_height\").alias(\"maior_bloco\")\n",
    ").first()\n",
    "# --> Calcula o menor e maior valor de \"block_height\" no DataFrame / Calculates the minimum and maximum \"block_height\" values in the DataFrame <--\n",
    "\n",
    "print(f\"Menor bloco salvo: {agg_blocks['menor_bloco']}\")\n",
    "print(f\"Maior bloco salvo: {agg_blocks['maior_bloco']}\")\n",
    "print(f\"Número de linhas em df_blocks: {df_blocks.count()}\")\n",
    "# --> Imprime o menor e maior bloco e o número total de linhas no DataFrame / Prints the smallest and largest block and the total number of rows in the DataFrame <--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_price_dolar.select(\"price\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blocks.select( \"block_height\").describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_price_dolar.show(10)\n",
    "df_price_dolar.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Configurar a conta de serviço\n",
    "# os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/Users/rodrigocampos/Documents/Bitcoin/btcanalytics-453021-dc83ec3411cb.json\"\n",
    "\n",
    "# # Configurar pipeline\n",
    "# class MyOptions(PipelineOptions):\n",
    "#     @classmethod\n",
    "#     def _add_argparse_args(cls, parser):\n",
    "#         parser.add_argument('--input_parquet')\n",
    "#         parser.add_argument('--output_table')\n",
    "\n",
    "# def run():\n",
    "#     options = MyOptions()\n",
    "#     gcloud_options = options.view_as(GoogleCloudOptions)\n",
    "#     gcloud_options.project = \"btcanalytics-453021\"\n",
    "#     gcloud_options.job_name = \"btc-blocks-job\"\n",
    "#     gcloud_options.staging_location = \"gs://btcanalytics-dataflow/staging\"\n",
    "#     gcloud_options.temp_location = \"gs://btcanalytics-dataflow/temp\"\n",
    "#     gcloud_options.region = \"us-central1\"\n",
    "#     options.view_as(SetupOptions).save_main_session = True\n",
    "#     options.view_as(GoogleCloudOptions).runner = \"DataflowRunner\"  # ou DirectRunner para teste local\n",
    "\n",
    "#     input_path = options.input_parquet\n",
    "#     output_table = options.output_table\n",
    "\n",
    "#     # Função para converter Parquet em dicts\n",
    "#     def read_parquet(file_path):\n",
    "#         table = pq.read_table(file_path)\n",
    "#         return table.to_pydict()\n",
    "\n",
    "#     with beam.Pipeline(options=options) as p:\n",
    "#         (\n",
    "#             p\n",
    "#             | \"ReadFile\" >> beam.Create([input_path])\n",
    "#             | \"ReadParquet\" >> beam.FlatMap(lambda path: [dict(zip(row.keys(), values)) for row in [read_parquet(path)] for values in zip(*row.values())])\n",
    "#             | \"WriteToBigQuery\" >> beam.io.WriteToBigQuery(\n",
    "#                 output_table,\n",
    "#                 write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,\n",
    "#                 create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED\n",
    "#             )\n",
    "#         )\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import apache_beam as beam\n",
    "# from apache_beam.options.pipeline_options import PipelineOptions, GoogleCloudOptions, SetupOptions\n",
    "# from google.cloud import bigquery\n",
    "# import os\n",
    "# import pyarrow.parquet as pq\n",
    "\n",
    "# # ==========================================================\n",
    "# # Configurar a conta de serviço / Set up service account\n",
    "# # ==========================================================\n",
    "# os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/Users/rodrigocampos/Documents/Bitcoin/btcanalytics-453021-dc83ec3411cb.json\"\n",
    "# # --> Define a variável de ambiente com as credenciais do Google Cloud / Sets environment variable for GCP credentials <--\n",
    "\n",
    "# # ==========================================================\n",
    "# # Configurar pipeline / Configure pipeline options\n",
    "# # ==========================================================\n",
    "# class MyOptions(PipelineOptions):\n",
    "#     @classmethod\n",
    "#     def _add_argparse_args(cls, parser):\n",
    "#         parser.add_argument('--input_parquet')\n",
    "#         parser.add_argument('--output_table')\n",
    "#         # --> Define os argumentos esperados na linha de comando / Defines CLI arguments expected for the pipeline <--\n",
    "\n",
    "# # ==========================================================\n",
    "# # Função principal da pipeline / Main pipeline function\n",
    "# # ==========================================================\n",
    "# def run():\n",
    "#     options = MyOptions()\n",
    "#     gcloud_options = options.view_as(GoogleCloudOptions)\n",
    "#     gcloud_options.project = \"btcanalytics-453021\"\n",
    "#     gcloud_options.job_name = \"btc-blocks-job\"\n",
    "#     gcloud_options.staging_location = \"gs://btcanalytics-dataflow/staging\"\n",
    "#     gcloud_options.temp_location = \"gs://btcanalytics-dataflow/temp\"\n",
    "#     gcloud_options.region = \"us-central1\"\n",
    "#     # --> Configura opções do Dataflow no Google Cloud / Configures Dataflow job options on GCP <--\n",
    "\n",
    "#     options.view_as(SetupOptions).save_main_session = True\n",
    "#     options.view_as(GoogleCloudOptions).runner = \"DataflowRunner\"\n",
    "#     # --> Define o runner como Dataflow para execução na nuvem / Sets the runner as Dataflow for cloud execution <--\n",
    "\n",
    "#     input_path = options.input_parquet\n",
    "#     output_table = options.output_table\n",
    "#     # --> Lê os argumentos de entrada (parquet) e saída (BigQuery) / Reads input and output arguments <--\n",
    "\n",
    "#     # ==========================================================\n",
    "#     # Função para ler Parquet localmente / Read Parquet file\n",
    "#     # ==========================================================\n",
    "#     def read_parquet(file_path):\n",
    "#         table = pq.read_table(file_path)\n",
    "#         return table.to_pydict()\n",
    "#         # --> Lê o arquivo Parquet e converte para dicionário Python / Reads Parquet and converts to Python dictionary <--\n",
    "\n",
    "#     with beam.Pipeline(options=options) as p:\n",
    "#         (\n",
    "#             p\n",
    "#             | \"ReadFile\" >> beam.Create([input_path])\n",
    "#             # --> Cria o PCollection com caminho do arquivo / Creates PCollection from the parquet path <--\n",
    "\n",
    "#             | \"ReadParquet\" >> beam.FlatMap(\n",
    "#                 lambda path: [dict(zip(row.keys(), values)) for row in [read_parquet(path)] for values in zip(*row.values())]\n",
    "#             )\n",
    "#             # --> Lê e transforma o conteúdo Parquet em dicionários linha por linha / Converts each row in Parquet to dicts <--\n",
    "\n",
    "#             | \"WriteToBigQuery\" >> beam.io.WriteToBigQuery(\n",
    "#                 output_table,\n",
    "#                 write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,\n",
    "#                 create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED\n",
    "#             )\n",
    "#             # --> Escreve os dados no BigQuery com append e criação se necessário / Writes data to BigQuery with append and create if needed <--\n",
    "#         )\n",
    "\n",
    "# # ==========================================================\n",
    "# # Execução principal / Main execution\n",
    "# # ==========================================================\n",
    "# if __name__ == \"__main__\":\n",
    "#     run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# Caminhos principais / Main paths\n",
    "# ==========================================================\n",
    "CHECKPOINT_FILE = \"/Users/rodrigocampos/Library/Mobile Documents/com~apple~CloudDocs/bitcoin_features/block_range_checkpoint.txt\"\n",
    "SAVE_PATH = \"/Users/rodrigocampos/Library/Mobile Documents/com~apple~CloudDocs/bitcoin_features/features_temp/df_bitcoin_features\"\n",
    "# --> Caminho de destino para os dados processados / Path to store processed feature data <--\n",
    "PATH_BLOCKS = \"/Users/rodrigocampos/Library/Mobile Documents/com~apple~CloudDocs/bitcoin_features/blockchain_blocks_part\"\n",
    "PATH_TRANSACTIONS = \"/Users/rodrigocampos/Library/Mobile Documents/com~apple~CloudDocs/bitcoin_features/blockchain_transactions_part\"\n",
    "PATH_ADDRESSES = \"/Users/rodrigocampos/Library/Mobile Documents/com~apple~CloudDocs/bitcoin_features/blockchain_addresses_part\"\n",
    "USD_PATH = \"/Users/rodrigocampos/Library/Mobile Documents/com~apple~CloudDocs/bitcoin_features/features_dolar_parquet\"\n",
    "\n",
    "# ==========================================================\n",
    "# Cria diretórios se não existirem / Create directories if not exist\n",
    "# ==========================================================\n",
    "for path in [PATH_BLOCKS, PATH_TRANSACTIONS, PATH_ADDRESSES]:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        print(f\"[INFO] Diretório criado: {path}\")\n",
    "# --> Cria diretórios obrigatórios para os dados de entrada se não existirem / Creates required folders <--\n",
    "\n",
    "# ==========================================================\n",
    "# Funções auxiliares / Helper functions\n",
    "# ==========================================================\n",
    "def update_checkpoint(path, min_block, max_block):\n",
    "    with open(path, \"w\") as f:\n",
    "        f.write(f\"{min_block},{max_block}\")\n",
    "# --> Atualiza o arquivo de checkpoint com os blocos mínimo e máximo / Updates the checkpoint file with new range <--\n",
    "\n",
    "def read_checkpoint(path):\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r\") as f:\n",
    "            content = f.read().strip()\n",
    "            if content:\n",
    "                lines = content.split(\",\")\n",
    "                if len(lines) == 2:\n",
    "                    return int(lines[0]), int(lines[1])\n",
    "    # --> Caso o arquivo não exista, tenta inferir pelos arquivos salvos / Infers checkpoint if file is missing <--\n",
    "    parquet_files = glob.glob(os.path.join(SAVE_PATH, \"**\", \"*.parquet\"), recursive=True)\n",
    "    if parquet_files:\n",
    "        try:\n",
    "            df_existing = spark.read.option(\"basePath\", SAVE_PATH).parquet(SAVE_PATH)\n",
    "            min_block = df_existing.select(F.min(\"block_height\")).first()[0]\n",
    "            max_block = df_existing.select(F.max(\"block_height\")).first()[0]\n",
    "            print(f\"[INFO] Checkpoint inferido do Parquet: {min_block} - {max_block}\")\n",
    "            update_checkpoint(path, min_block, max_block)\n",
    "            return min_block, max_block\n",
    "        except Exception as e:\n",
    "            print(f\"[ERRO] Falha ao ler Parquet: {e}\")\n",
    "    print(\"[INFO] Nenhum arquivo parquet encontrado ainda. Iniciando do zero.\")\n",
    "    update_checkpoint(path, 0, 0)\n",
    "    return 0, 0\n",
    "\n",
    "# --> Verifica se a partição do ano/mês já existe / Checks if the year/month partition already exists <--\n",
    "def partition_exists(year, month, base_path):\n",
    "    path = os.path.join(base_path, f\"year_block={year}/month_block={month}\")\n",
    "    return os.path.exists(path) and any(os.scandir(path))\n",
    "\n",
    "# ==========================================================\n",
    "# Pipeline principal / Main pipeline\n",
    "# ==========================================================\n",
    "def process_full_range_blocks():\n",
    "    min_saved, max_saved = read_checkpoint(CHECKPOINT_FILE)\n",
    "    print(f\"[INFO] Blocos salvos anteriormente: {min_saved} - {max_saved}\")\n",
    "\n",
    "    try:\n",
    "        df_blocks = spark.read.option(\"basePath\", PATH_BLOCKS).parquet(PATH_BLOCKS)\n",
    "        df_transactions = spark.read.option(\"basePath\", PATH_TRANSACTIONS).parquet(PATH_TRANSACTIONS)\n",
    "        df_addresses = spark.read.option(\"basePath\", PATH_ADDRESSES).parquet(PATH_ADDRESSES)\n",
    "        df_price_dolar = spark.read.parquet(USD_PATH)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERRO] Falha ao ler os arquivos Parquet: {e}\")\n",
    "        return\n",
    "\n",
    "    filter_condition = (F.col(\"block_height\") < min_saved) | (F.col(\"block_height\") > max_saved)\n",
    "    df_blocks_new = df_blocks.filter(filter_condition)\n",
    "    df_transactions_new = df_transactions.filter(filter_condition)\n",
    "    df_addresses_new = df_addresses.filter(filter_condition)\n",
    "    # --> Seleciona apenas os blocos que ainda não foram salvos / Selects only blocks not previously saved <--\n",
    "\n",
    "    if df_blocks_new.rdd.isEmpty():\n",
    "        print(\"[INFO] Nenhum bloco novo ou retroativo para processar.\")\n",
    "        return\n",
    "\n",
    "    if \"total_btc_moved\" in df_blocks_new.columns:\n",
    "        df_blocks_new = df_blocks_new.withColumnRenamed(\"total_btc_moved\", \"total_btc_moved_blocks\")\n",
    "\n",
    "    df_blocks_new = df_blocks_new.withColumn(\"year_block\", F.year(\"block_timestamp\")) \\\n",
    "                                 .withColumn(\"month_block\", F.month(\"block_timestamp\")) \\\n",
    "                                 .withColumn(\"block_ts_long\", F.col(\"block_timestamp\").cast(\"long\"))\n",
    "\n",
    "    df_transactions_agg = df_transactions_new.groupBy(\"block_height\").agg(\n",
    "        F.sum(\"total_input\").alias(\"total_input_sum\"),\n",
    "        F.sum(\"total_output\").alias(\"total_output_sum\"),\n",
    "        F.sum(\"fee\").alias(\"total_fees_tx\"),\n",
    "        F.avg(\"fee\").alias(\"avg_fee_per_tx\"),\n",
    "        F.count(\"tx_hash\").alias(\"num_transactions\"),\n",
    "        F.avg(\"total_input\").alias(\"avg_input_per_tx\"),\n",
    "        F.avg(\"total_output\").alias(\"avg_output_per_tx\"),\n",
    "        F.avg(\"transaction_size\").alias(\"avg_tx_size\")\n",
    "    )\n",
    "\n",
    "    df_addresses_agg = df_addresses_new.groupBy(\"block_height\").agg(\n",
    "        F.sum(F.when(F.col(\"direction\") == \"input\", F.col(\"amount\")).otherwise(0)).alias(\"total_input_btc_addr\"),\n",
    "        F.sum(F.when(F.col(\"direction\") == \"output\", F.col(\"amount\")).otherwise(0)).alias(\"total_output_btc_addr\"),\n",
    "        F.countDistinct(\"address\").alias(\"unique_addresses\"),\n",
    "        F.count(F.when(F.col(\"is_zero\"), 1)).alias(\"zero_balance_addresses\"),\n",
    "        F.count(F.when(F.col(\"wallet_type\") == \"Multisig\", 1)).alias(\"multisig_wallets\")\n",
    "    )\n",
    "\n",
    "    df_price_dolar = df_price_dolar.withColumnRenamed(\"date\", \"price_date\") \\\n",
    "                                   .withColumnRenamed(\"price\", \"btc_price_usd\") \\\n",
    "                                   .withColumn(\"price_ts_long\", F.col(\"price_date\").cast(\"long\"))\n",
    "\n",
    "    df_join = df_blocks_new.join(\n",
    "        broadcast(df_price_dolar),\n",
    "        on=F.col(\"block_ts_long\") >= F.col(\"price_ts_long\"),\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    window_price = Window.partitionBy(\"block_height\").orderBy(F.abs(F.col(\"block_ts_long\") - F.col(\"price_ts_long\")))\n",
    "    df_blocks_price = df_join.withColumn(\"rank\", F.row_number().over(window_price)) \\\n",
    "                             .filter(F.col(\"rank\") == 1) \\\n",
    "                             .drop(\"rank\", \"price_ts_long\", \"block_ts_long\")\n",
    "\n",
    "    df_final = df_blocks_price.alias(\"b\") \\\n",
    "        .join(df_transactions_agg.alias(\"t\"), \"block_height\", \"left\") \\\n",
    "        .join(df_addresses_agg.alias(\"a\"), \"block_height\", \"left\") \\\n",
    "        .select(\n",
    "            \"b.block_height\", \"b.block_timestamp\", \"b.total_btc_moved_blocks\",\n",
    "            \"b.year_block\", \"b.month_block\",\n",
    "            \"t.num_transactions\", \"t.total_fees_tx\", \"t.avg_fee_per_tx\",\n",
    "            \"t.avg_input_per_tx\", \"t.avg_output_per_tx\", \"t.avg_tx_size\",\n",
    "            \"a.total_input_btc_addr\", \"a.total_output_btc_addr\", \"a.unique_addresses\",\n",
    "            \"a.zero_balance_addresses\", \"a.multisig_wallets\",\n",
    "            \"b.btc_price_usd\"\n",
    "        ).dropDuplicates([\"block_height\"])\n",
    "\n",
    "    unique_partitions = df_final.select(\"year_block\", \"month_block\").dropDuplicates().collect()\n",
    "    for row in unique_partitions:\n",
    "        year = row[\"year_block\"]\n",
    "        month = row[\"month_block\"]\n",
    "\n",
    "        if partition_exists(year, month, SAVE_PATH):\n",
    "            print(f\"[INFO] Particao ja existe: {year}/{month} — pulando.\")\n",
    "            continue\n",
    "\n",
    "        df_part = df_final.filter((F.col(\"year_block\") == year) & (F.col(\"month_block\") == month))\n",
    "        df_part.write.mode(\"overwrite\") \\\n",
    "                      .option(\"compression\", \"snappy\") \\\n",
    "                      .partitionBy(\"year_block\", \"month_block\") \\\n",
    "                      .parquet(SAVE_PATH)\n",
    "        print(f\"[SALVO] Particao: {year}/{month}\")\n",
    "\n",
    "    new_min = df_final.select(F.min(\"block_height\")).first()[0]\n",
    "    new_max = df_final.select(F.max(\"block_height\")).first()[0]\n",
    "\n",
    "    if new_min is not None and new_max is not None:\n",
    "        updated_min = min(min_saved, new_min) if min_saved else new_min\n",
    "        updated_max = max(max_saved, new_max) if max_saved else new_max\n",
    "        update_checkpoint(CHECKPOINT_FILE, updated_min, updated_max)\n",
    "        print(f\"[SUCESSO] Blocos salvos: {new_min} até {new_max}\")\n",
    "    else:\n",
    "        print(\"[INFO] Nenhum bloco válido processado.\")\n",
    "\n",
    "# ==========================================================\n",
    "# Execução principal / Main execution\n",
    "# ==========================================================\n",
    "if __name__ == \"__main__\":\n",
    "    process_full_range_blocks()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# Caminhos principais / Paths\n",
    "# ==========================================================\n",
    "BASE_PATH = \"/Users/rodrigocampos/Library/Mobile Documents/com~apple~CloudDocs/bitcoin_features\"\n",
    "CHECKPOINT_FILE = os.path.join(BASE_PATH, \"block_range_checkpoint.txt\")\n",
    "PATH_BLOCKS = os.path.join(BASE_PATH, \"blockchain_blocks_part\")\n",
    "PATH_TRANSACTIONS = os.path.join(BASE_PATH, \"blockchain_transactions_part\")\n",
    "PATH_ADDRESSES = os.path.join(BASE_PATH, \"blockchain_addresses_part\")\n",
    "\n",
    "# ==========================================================\n",
    "# Lê intervalo salvo no checkpoint\n",
    "# Read saved interval from checkpoint file\n",
    "# ==========================================================\n",
    "def read_checkpoint(path):\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r\") as f:\n",
    "            content = f.read().strip()\n",
    "            if content:\n",
    "                lines = content.split(\",\")\n",
    "                if len(lines) == 2:\n",
    "                    return int(lines[0]), int(lines[1])\n",
    "    return None, None\n",
    "    # --> Retorna None se o checkpoint não existir ou estiver incompleto / Returns None if checkpoint is missing or incomplete <--\n",
    "\n",
    "# ==========================================================\n",
    "# Função para deletar arquivos já salvos\n",
    "# Deletes saved block ranges based on checkpoint interval\n",
    "# ==========================================================\n",
    "def delete_saved_blocks():\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"DeleteSavedBlocks\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "        .getOrCreate()\n",
    "    # --> Inicializa sessão Spark local para manipulação de dados / Starts local Spark session for data manipulation <--\n",
    "\n",
    "    saved_min, saved_max = read_checkpoint(CHECKPOINT_FILE)\n",
    "    if saved_min is None or saved_max is None:\n",
    "        print(\"[INFO] Nenhum intervalo salvo para excluir.\")\n",
    "        return\n",
    "    # --> Interrompe se nenhum intervalo válido for encontrado / Exit if no valid interval found <--\n",
    "\n",
    "    print(f\"[INFO] Excluindo blocos salvos de {saved_min} até {saved_max}...\")\n",
    "\n",
    "    # --> Processa cada categoria de dados: blocos, transações e endereços / Process each data category: blocks, transactions, addresses <--\n",
    "    for name, path in [(\"blocos\", PATH_BLOCKS), (\"transações\", PATH_TRANSACTIONS), (\"endereços\", PATH_ADDRESSES)]:\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"[INFO] Pasta de {name} não encontrada: {path}\")\n",
    "            continue\n",
    "        # --> Pula se o caminho não existir / Skip if the path does not exist <--\n",
    "\n",
    "        df = spark.read.option(\"basePath\", path).parquet(path)\n",
    "        df_to_keep = df.filter((F.col(\"block_height\") < saved_min) | (F.col(\"block_height\") > saved_max))\n",
    "        # --> Filtra apenas blocos fora do intervalo salvo / Keep only blocks outside the saved interval <--\n",
    "\n",
    "        if df_to_keep.rdd.isEmpty():\n",
    "            print(f\"[INFO] Todos os {name} foram salvos. Excluindo diretório {path}...\")\n",
    "            shutil.rmtree(path)\n",
    "            print(f\"[SUCESSO] Diretório de {name} excluído.\")\n",
    "            # --> Remove diretório inteiro se não houver nada a preservar / Remove entire directory if nothing to preserve <--\n",
    "        else:\n",
    "            print(f\"[INFO] Substituindo {name} com blocos não salvos...\")\n",
    "            temp_path = path + \"_temp\"\n",
    "            df_to_keep.write.mode(\"overwrite\").option(\"compression\", \"snappy\").parquet(temp_path)\n",
    "            shutil.rmtree(path)\n",
    "            os.rename(temp_path, path)\n",
    "            print(f\"[SUCESSO] {name.capitalize()} salvos removidos, restantes preservados.\")\n",
    "            # --> Substitui o diretório original mantendo apenas blocos fora do intervalo salvo / Replaces original directory with only unsaved blocks <--\n",
    "\n",
    "    spark.stop()\n",
    "    # --> Encerra a sessão Spark após finalização / Stops Spark session after completion <--\n",
    "\n",
    "# ==========================================================\n",
    "# Execução principal\n",
    "# Main execution\n",
    "# ==========================================================\n",
    "if __name__ == \"__main__\":\n",
    "    delete_saved_blocks()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
