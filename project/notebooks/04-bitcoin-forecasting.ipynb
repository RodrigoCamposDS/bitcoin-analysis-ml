{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum√°rio T√©cnico ‚Äî Previs√£o do Pre√ßo do Bitcoin\n",
    "\n",
    "##### 1. Setup Inicial\n",
    "- [1.1 Importa√ß√£o das Bibliotecas](#11-Importacao-das-Bibliotecas)\n",
    "- [1.2 Inicializa√ß√£o da Sess√£o Spark](#12-inicializacao-da-sessao-spark) \n",
    "- [1.3 Carregamento dos Dados](#13-carregamento-dos-dados)\n",
    "\n",
    "\n",
    "##### 2. Auditoria e Valida√ß√£o da S√©rie Temporal\n",
    "- [2.1 An√°lise Estrutural do DataFrame](#21-analise-estrutural-do-dataframe)\n",
    "- [2.2 An√°lise Temporal de Integridade](#22-analise-temporal-de-integridade)\n",
    "\n",
    "##### 3. An√°lise Explorat√≥ria da S√©rie Temporal (EDA)\n",
    "- [3.1 Transforma√ß√£o Temporal](#31-transforma√ß√£o-temporal)\n",
    "- [3.2 Visualiza√ß√µes Temporais](#32-visualiza√ß√µes-temporais)\n",
    "\n",
    "##### 4. Engenharia de Features Temporais\n",
    "- [4.1 Retornos e Volatilidade](#41-retornos-e-volatilidade)\n",
    "- [4.2 Decomposi√ß√£o Estrutural da S√©rie (STL)](#42-decomposicao-estrutural-da-serie-stl)\n",
    "- [4.3 An√°lise Espectral (FFT)](#43-an√°lise-espectral-fft)\n",
    "- [4.4 Detec√ß√£o de Padr√µes C√≠clicos](#44-detec√ß√£o-de-padr√µes-c√≠clicos)\n",
    "- [4.5 Estrutura e Persist√™ncia Temporal](#45-estrutura-e-persistencia-temporal)\n",
    "\n",
    "##### 5. An√°lise de Estacionariedade e Transforma√ß√µes\n",
    "- [5.1 Transforma√ß√µes da S√©rie](#51-transforma√ß√µes-da-s√©rie)\n",
    "- [5.2 Diagn√≥stico de Estacionariedade](#52-diagn√≥stico-de-estacionariedade)\n",
    "\n",
    "##### 6. Pr√©-Modelagem e Ajustes Estat√≠sticos\n",
    "- [6.1 Modelos Lineares de Benchmark (ARIMA)](#61-modelos-lineares-de-benchmark-arima)\n",
    "- [6.2 Diagn√≥sticos de Res√≠duos](#62-diagn√≥sticos-de-res√≠duos)\n",
    "\n",
    "##### 7. Consolida√ß√£o das Features Temporais\n",
    "- [Features STL](#features-stl)\n",
    "- [Features de Retorno](#features-de-retorno)\n",
    "- [Features de Volatilidade](#features-de-volatilidade)\n",
    "- [Features ACF/PACF](#features-acfpacf)\n",
    "- [Features de Frequ√™ncia (FFT)](#features-de-frequ√™ncia-fft)\n",
    "- [Features de Regime](#features-de-regime)\n",
    "- [Features C√≠clicas](#features-c√≠clicas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup Inicial\n",
    "#### 1.1 Importacao das Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import plotly.express as px\n",
    "from pyspark.sql.window import Window\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from plotly.subplots import make_subplots\n",
    "from pyspark.sql.functions import col, unix_timestamp\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from scipy.signal import find_peaks, detrend\n",
    "from scipy.fft import fft, fftfreq\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import warnings\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "import plotly.graph_objects as go\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "import scipy.stats as stats\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from src.visualizations.plot_btc import plot_btc_price_timeseries,plot_transformed_series, plot_seasonal_weekly_line, plot_rolling_diagnostics_overlay, plot_btc_boxplot_by_week_comparison, plot_btc_boxplot_by_week, plot_btc_boxplot_by_month_comparison, plot_bitcoin_seasonal_patterns, plot_intraday_price_by_hour,  plot_weekly_seasonality_all_years, plot_seasonal_daily_line, plot_stl_decomposition, plot_fft_spectrum, plot_btc_boxplot_by_dayofyear, plot_histogram_variacao_btc, plot_series_comparativa, plot_acf_diferenciada,plot_rolling_mean_std, plot_btc_boxplot_by_month, plot_acf_pacf, plot_arima_layers, plot_acf_residuos, plot_volatility_rolling,plot_residuos_analysis, plot_btc_candlestick_ohlc, plot_acf_pacf_returns, plot_btc_boxplot_by_hour, plot_log_return_analysis\n",
    "from src.features.stl_features import extract_stl_features\n",
    "from src.features.arima_features import extract_arima_features\n",
    "from src.features.regime_features import extract_regime_features\n",
    "from src.features.fft_features import extract_fft_features, extract_peak_features, extract_cycle_features, reconstruct_fft, extract_cyclic_features\n",
    "from scipy.stats import normaltest\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "import pandas as pd\n",
    "from scipy.signal import find_peaks\n",
    "import numpy as np\n",
    "from hurst import compute_Hc\n",
    "from scipy.stats import median_abs_deviation\n",
    "import numpy as np\n",
    "from hurst import compute_Hc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.stats.diagnostic import het_arch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import entropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Inicializacao da Sessao Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INICIAR SESS√ÉO SPARK COM OTIMIZA√á√ïES PARA O MAC M2 (8GB RAM)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Bitcoin_Forecasting\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.default.parallelism\", \"8\") \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .config(\"spark.executor.memory\", \"5g\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.85\") \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"128MB\") \\\n",
    "    .config(\"spark.cleaner.referenceTracking.cleanCheckpoints\", \"false\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"60000ms\") \\\n",
    "    .config(\"spark.task.cpus\", \"2\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# REDUZIR LOGS PARA EVITAR POLUI√á√ÉO NO CONSOLE\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(\"\\n SparkSession configurada com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Carregamento dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PASTA_FEATURES = \"/Users/rodrigocampos/Library/Mobile Documents/com~apple~CloudDocs/bitcoin_features/features_temp/df_bitcoin_features.parquet\"\n",
    "df_features_temp = spark.read.parquet(PASTA_FEATURES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Auditoria e Valida√ß√£o da S√©rie Temporal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 An√°lise Estrutural do DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üî∂ ‚Çø -----> Tipos de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features_temp.printSchema(  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üî∂ ‚Çø -----> Contagem de Linhas e Colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_linhas = df_features_temp.count()  \n",
    "# --> Counts the total number of rows in the DataFrame / Conta o n√∫mero total de linhas no DataFrame <--\n",
    "\n",
    "num_colunas = len(df_features_temp.columns)  \n",
    "# --> Counts the total number of columns in the DataFrame / Conta o n√∫mero total de colunas no DataFrame <--\n",
    "\n",
    "print(f\"df_features_temp possui {num_linhas} linhas e {num_colunas} colunas.\")  \n",
    "# --> Prints the shape of the DataFrame / Imprime a forma (dimens√£o) do DataFrame <--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üî∂ ‚Çø -----> Verifica√ß√£o de Valores Ausentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulos = df_features_temp.filter(df_features_temp[\"btc_price_usd\"].isNull())\n",
    "# --> Filters rows where 'btc_price_usd' is null / Filtra as linhas onde 'btc_price_usd' est√° nulo <--\n",
    "\n",
    "nulos.select(\"block_height\", \"block_timestamp\").show(truncate=False)\n",
    "# --> Shows block height and timestamp for null-price rows / Mostra a altura do bloco e o timestamp para os pre√ßos nulos <--\n",
    "\n",
    "print(\"Total de blocos com pre√ßo nulo:\", nulos.count())\n",
    "# --> Counts how many rows have missing prices / Conta quantas linhas t√™m pre√ßos ausentes <--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üî∂ ‚Çø -----> Vizualiza√ß√£o do Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Analise Temporal de Integridade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üî∂ ‚Çø -----> Primeira e √öltima Data (block_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular o timestamp m√≠nimo e m√°ximo\n",
    "df_features_temp.select(\n",
    "    F.min(\"block_timestamp\").alias(\"Min block_timestamp\"),\n",
    "    F.max(\"block_timestamp\").alias(\"Max block_timestamp\")\n",
    ").show(truncate=False)\n",
    "# --> Selects and displays the minimum and maximum timestamps in the 'block_timestamp' column / \n",
    "# --> Seleciona e exibe os timestamps m√≠nimo e m√°ximo da coluna 'block_timestamp' <--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üî∂ ‚Çø -----> Primeiro e √öltimo Bloco (block_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primeiro_bloco = df_features_temp.select(\"block_height\").orderBy(\"block_height\").first()[0]\n",
    "# --> Gets the smallest block height (first block) from the DataFrame / \n",
    "# --> Obt√©m a menor altura de bloco (primeiro bloco) do DataFrame <--\n",
    "\n",
    "ultimo_bloco = df_features_temp.select(\"block_height\").orderBy(F.desc(\"block_height\")).first()[0]\n",
    "# --> Gets the largest block height (last block) from the DataFrame / \n",
    "# --> Obt√©m a maior altura de bloco (√∫ltimo bloco) do DataFrame <--\n",
    "\n",
    "print(f\"Primeiro bloco salvo: {primeiro_bloco}\")\n",
    "print(f\"√öltimo bloco salvo: {ultimo_bloco}\")\n",
    "# --> Prints the range of saved blocks / Imprime o intervalo de blocos salvos <--\n",
    "\n",
    "print(df_features_temp.select(\"block_height\").distinct().count())\n",
    "# --> Counts the number of distinct block heights in the DataFrame / \n",
    "# --> Conta o n√∫mero de alturas de blocos distintas no DataFrame <--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üî∂ ‚Çø -----> Gaps de Blocos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar cada bloco com o anterior\n",
    "\n",
    "windowSpec = Window.orderBy(\"block_height\")\n",
    "# --> Define a window sorted by block height / Define uma janela ordenada pela altura dos blocos <--\n",
    "\n",
    "df_blocks = df_features_temp.withColumn(\"prev_block_height\", F.lag(\"block_height\").over(windowSpec))\n",
    "# --> Creates a column with the previous block height / Cria uma coluna com a altura do bloco anterior <--\n",
    "\n",
    "df_blocks_filtered = df_blocks.select(\"block_height\", \"prev_block_height\")\n",
    "# --> Select only relevant columns before checking for gaps / Seleciona apenas as colunas relevantes antes de verificar gaps <--\n",
    "\n",
    "df_gaps = df_blocks_filtered.withColumn(\"gap_detected\", (col(\"block_height\") - col(\"prev_block_height\")) > 1)\n",
    "# --> Creates a boolean column that marks if a gap exists between blocks / Cria uma coluna booleana que marca se h√° gap entre os blocos <--\n",
    "\n",
    "df_gaps_filtered = df_gaps.filter(col(\"gap_detected\") == True)\n",
    "# --> Filters only the rows where a gap was detected / Filtra apenas as linhas onde um gap foi detectado <--\n",
    "\n",
    "df_gaps_filtered.select(\"prev_block_height\", \"block_height\").show()\n",
    "# --> Displays the previous and current block height where a gap occurred / Mostra os blocos com gaps detectados <--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üî∂ ‚Çø -----> Gaps de Tempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Janela ordenada por block_height\n",
    "windowSpec = Window.orderBy(\"block_height\")\n",
    "# --> Define a window ordered by block height / Define uma janela ordenada pela altura dos blocos <--\n",
    "\n",
    "df_with_prev_ts = df_features_temp.withColumn(\n",
    "    \"prev_timestamp\", F.lag(\"block_timestamp\").over(windowSpec)\n",
    ")\n",
    "# --> Creates a new column with the previous block's timestamp / Cria uma nova coluna com o timestamp do bloco anterior <--\n",
    "\n",
    "df_with_diff_days = df_with_prev_ts.withColumn(\n",
    "    \"gap_days\",\n",
    "    (F.unix_timestamp(\"block_timestamp\") - F.unix_timestamp(\"prev_timestamp\")) / (60 * 60 * 24)\n",
    ")\n",
    "# --> Calculates the time difference in days between consecutive blocks / Calcula a diferen√ßa em dias entre blocos consecutivos <--\n",
    "\n",
    "df_with_day_gaps = df_with_diff_days.withColumn(\n",
    "    \"gap_detected_days\", col(\"gap_days\") > 1\n",
    ")\n",
    "# --> Creates a boolean column to flag gaps greater than 1 day / Cria uma coluna booleana para marcar gaps maiores que 1 dia <--\n",
    "\n",
    "df_day_gaps = df_with_day_gaps.filter(col(\"gap_detected_days\") == True)\n",
    "# --> Filters only rows with time gaps larger than 1 day / Filtra apenas as linhas com gaps de tempo maiores que 1 dia <--\n",
    "\n",
    "df_day_gaps.select(\"prev_timestamp\", \"block_timestamp\", \"gap_days\", \"block_height\").show(truncate=False)\n",
    "# --> Displays previous and current timestamps, gap in days, and block height / Exibe timestamps anterior e atual, gap em dias e altura do bloco <--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. An√°lise Explorat√≥ria da S√©rie Temporal (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Transformacao Temporal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üî∂ ‚Çø -----> Convers√£o de Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time = df_features_temp.toPandas()\n",
    "# --> Converte o DataFrame PySpark para Pandas / Converts PySpark DataFrame to Pandas <--\n",
    "\n",
    "df_time[\"block_timestamp\"] = pd.to_datetime(df_time[\"block_timestamp\"])\n",
    "# --> Garante que o campo de tempo seja do tipo datetime / Ensures timestamp field is datetime type <--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üî∂ ‚Çø -----> Defini√ß√£o de √çndice Temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_index = df_time.sort_values(\"block_timestamp\")\n",
    "# --> Ordena o DataFrame pelo tempo / Sorts the DataFrame by timestamp <--\n",
    "\n",
    "df_time_index.set_index('block_timestamp', inplace=True)\n",
    "# --> Define o √≠ndice como timestamp (necess√°rio para o resample) / Sets timestamp as index (needed for resample) <--\n",
    "\n",
    "print(df_time_index.index)\n",
    "# --> Mostra o novo √≠ndice temporal / Displays new datetime index <--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_index.index.to_series().diff().value_counts()\n",
    "# --> Calcula a diferen√ßa entre √≠ndices consecutivos e conta a frequ√™ncia de cada intervalo /\n",
    "# --> Computes the difference between consecutive index entries and counts frequency of each interval <--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üî∂ ‚Çø -----> Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_index = df_time_index.resample('h').mean()\n",
    "# --> Reamostra os dados com frequ√™ncia hor√°ria, calculando a m√©dia por hora / Resamples data to hourly frequency, taking the mean per hour <--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Visualiza√ß√µes Temporais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üî∂ ‚Çø -----> S√©rie Temporal do Pre√ßo (USD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"/Users/rodrigocampos/Documents/Bitcoin/project/src/visualizations/BTC_black.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_btc_price_timeseries(df_time_index, image_path, resample='1h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üî∂ ‚Çø -----> Gr√°fico de Abertura vs. Fechamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_btc_candlestick_ohlc(df_time_index, image_path, resample='h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üî∂ ‚Çø -----> Boxplot por Janela Temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_btc_boxplot_by_hour(df_time_index, image_path)\n",
    "plot_btc_boxplot_by_dayofyear(df_time_index, image_path)\n",
    "plot_btc_boxplot_by_week(df_time_index, image_path)\n",
    "plot_btc_boxplot_by_month(df_time_index, image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üî∂ ‚Çø -----> Distribui√ß√£o e Histograma da Varia√ß√£o Percentual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram_variacao_btc(df_time_index, image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Engenharia de Features Temporais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Retornos e Volatilidade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Retornos e Volatilidade\n",
    "\n",
    "Retornos Logar√≠tmicos\n",
    "\n",
    "Dado o pre√ßo de um ativo $ P_t $ no tempo $ t $, o **retorno logar√≠tmico** √© definido como:\n",
    "\n",
    "$\n",
    "r_t = \\ln\\left(\\frac{P_t}{P_{t-1}}\\right)\n",
    "$\n",
    "\n",
    "Onde:\n",
    "\n",
    "- $ r_t $: retorno no instante $ t $\n",
    "- $ \\ln $: logaritmo natural\n",
    "- $ P_t $: pre√ßo do ativo no tempo $ t $\n",
    "- $ P_{t-1} $: pre√ßo do ativo no tempo anterior\n",
    "\n",
    "Este tipo de retorno √© amplamente utilizado por possuir propriedades aditivas no tempo, o que facilita a modelagem estat√≠stica.\n",
    "\n",
    "---\n",
    "\n",
    "Volatilidade Hist√≥rica\n",
    "\n",
    "A **volatilidade hist√≥rica** representa a dispers√£o dos retornos e √© calculada como o desvio padr√£o $ \\sigma $ dos retornos $ r_t $:\n",
    "\n",
    "$\n",
    "\\sigma = \\sqrt{\\frac{1}{N - 1} \\sum_{t=1}^{N} (r_t - \\bar{r})^2}\n",
    "$\n",
    "\n",
    "Onde:\n",
    "\n",
    "- $ sigma $: volatilidade (desvio padr√£o dos retornos)\n",
    "- $ N $: n√∫mero de observa√ß√µes\n",
    "- $ \\bar{r} $: m√©dia dos retornos\n",
    "\n",
    "---\n",
    "\n",
    "Interpreta√ß√£o:\n",
    "\n",
    "- **Retornos positivos** indicam valoriza√ß√£o do ativo.\n",
    "- **Volatilidade alta** sugere maior risco (grandes varia√ß√µes nos pre√ßos).\n",
    "- Pode-se utilizar janelas m√≥veis para calcular **volatilidade din√¢mica** ao longo do tempo.\n",
    "\n",
    "---\n",
    "\n",
    ">Observa√ß√µes adicionais:\n",
    "- A volatilidade anualizada pode ser obtida multiplicando a volatilidade di√°ria por $ \\sqrt{252} $, assumindo 252 preg√µes por ano:\n",
    "  $\n",
    "  \\sigma_{\\text{anual}} = \\sigma_{\\text{di√°ria}} \\cdot \\sqrt{252}\n",
    "$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_return = df_features_temp.toPandas()\n",
    "# --> Converte o DataFrame PySpark para Pandas / Converts PySpark DataFrame to Pandas <--\n",
    "\n",
    "df_return[\"block_timestamp\"] = pd.to_datetime(df_return[\"block_timestamp\"])\n",
    "# --> Garante que o campo de tempo seja do tipo datetime / Ensures timestamp field is datetime type <--\n",
    "\n",
    "df_return = df_return.sort_values(\"block_timestamp\")\n",
    "# --> Ordena o DataFrame pelo tempo / Sorts the DataFrame by timestamp <--\n",
    "\n",
    "df_return.set_index('block_timestamp', inplace=True)\n",
    "# --> Define o √≠ndice como timestamp (necess√°rio para o resample) / Sets timestamp as index (needed for resample) <--\n",
    "\n",
    "df_return = df_return.resample('d').mean()\n",
    "# --> Reamostra os dados com frequ√™ncia hor√°ria, calculando a m√©dia por hora / Resamples data to hourly frequency, taking the mean per hour <--\n",
    "\n",
    "print(df_return.index)\n",
    "# --> Mostra o novo √≠ndice temporal / Displays new datetime index <--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_return.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================================================================\n",
    "# EXTRA√á√ÉO DE RETORNOS E TARGETS / RETURN & TARGET FEATURE ENGINEERING\n",
    "# ================================================================\n",
    "def extract_return_features(\n",
    "    price_series: pd.Series,\n",
    "    block_height: pd.Series,\n",
    "    block_timestamp: pd.Series\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extrai o retorno logar√≠tmico e targets para modelagem preditiva.\n",
    "    / Extracts log returns and targets for predictive modeling.\n",
    "\n",
    "    Par√¢metros / Parameters:\n",
    "    - price_series: pd.Series\n",
    "        S√©rie temporal de pre√ßos / Time series of prices.\n",
    "    - block_height: pd.Series\n",
    "        Altura do bloco para manter rastreabilidade / Block height for traceability.\n",
    "    - block_timestamp: pd.Series\n",
    "        Carimbo de tempo do bloco / Block timestamp.\n",
    "\n",
    "    Retorna / Returns:\n",
    "    - pd.DataFrame com colunas: block_height, block_timestamp, log_return, target_direction, target_return\n",
    "      / DataFrame with features: block_height, timestamp, log return, binary and continuous targets.\n",
    "    \"\"\"\n",
    "\n",
    "    # ================================\n",
    "    # BASE TEMPORAL / BASE SETUP\n",
    "    # ================================\n",
    "    df = pd.DataFrame({\n",
    "        \"price\": price_series.values,\n",
    "        \"block_height\": block_height.values,\n",
    "        \"block_timestamp\": block_timestamp.values\n",
    "    })\n",
    "    # --> Cria DataFrame base com os metadados essenciais / Creates base DataFrame with essential metadata <--\n",
    "\n",
    "    # ================================\n",
    "    # ENGENHARIA DE RETORNOS / RETURN FEATURES\n",
    "    # ================================\n",
    "    df[\"log_return\"] = np.log(df[\"price\"] / df[\"price\"].shift(1))\n",
    "    # --> Calcula retorno logar√≠tmico entre per√≠odos / Calculates log return between periods <--\n",
    "\n",
    "    df[\"target_direction\"] = (df[\"log_return\"].shift(-1) > 0).astype(int)\n",
    "    # --> Define 1 se o pr√≥ximo retorno for positivo, 0 caso contr√°rio / Binary target: 1 if next return is positive <--\n",
    "\n",
    "    df[\"target_return\"] = df[\"log_return\"].shift(-1)\n",
    "    # --> Define o retorno futuro como vari√°vel cont√≠nua / Continuous target: next log return <--\n",
    "\n",
    "    # ================================\n",
    "    # LIMPEZA FINAL / CLEANUP\n",
    "    # ================================\n",
    "    df = df.dropna(subset=[\"log_return\", \"target_direction\", \"target_return\"])\n",
    "    # --> Remove valores nulos gerados pelos shifts / Drop NaNs caused by shifting <--\n",
    "\n",
    "    # ================================\n",
    "    # SA√çDA FINAL / FINAL OUTPUT\n",
    "    # ================================\n",
    "    return df[[\"block_height\", \"block_timestamp\", \"log_return\", \"target_direction\", \"target_return\"]]\n",
    "    # --> Retorna DataFrame final com colunas relevantes / Returns final DataFrame with relevant columns <--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_return = df_return.reset_index()\n",
    "\n",
    "df_returns = extract_return_features(\n",
    "    price_series=df_return[\"btc_price_usd\"],\n",
    "    block_height=df_return[\"block_height\"],\n",
    "    block_timestamp=df_return[\"block_timestamp\"]\n",
    ")\n",
    "\n",
    "# Visualizando o resultado\n",
    "print(\"DataFrame de Retornos e Targets:\")\n",
    "display(df_returns.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --> Estat√≠sticas descritivas dos retornos / Descriptive statistics of returns <--\n",
    "print(df_returns[\"log_return\"].describe())\n",
    "\n",
    "# --> Assimetria (skewness) da distribui√ß√£o / Skewness of the distribution <--\n",
    "print(f\"\\nSkew: {df_returns['log_return'].skew()}\")\n",
    "\n",
    "# --> Curtose (kurtosis) da distribui√ß√£o / Kurtosis of the distribution <--\n",
    "print(f\"\\nKurt: {df_returns['log_return'].kurt()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_log_return_analysis(df_returns[\"log_return\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf_pacf_returns(df_returns[\"log_return\"], 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_volatility_rolling(df_returns, window=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stat, p = normaltest(df_returns[\"log_return\"].dropna())\n",
    "print(f\"p-valor do teste de normalidade: {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ================================================================\n",
    "# EXTRA√á√ÉO DE FEATURES DE VOLATILIDADE / VOLATILITY FEATURE EXTRACTION\n",
    "# ================================================================\n",
    "def extract_volatility_features(\n",
    "    df: pd.DataFrame,\n",
    "    return_col: str = \"log_return\",\n",
    "    block_height_col: str = \"block_height\",\n",
    "    block_timestamp_col: str = \"block_timestamp\",\n",
    "    window: int = 24\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extrai features de volatilidade com base na s√©rie de retornos.\n",
    "    / Extracts volatility features based on the return series.\n",
    "\n",
    "    Par√¢metros / Parameters:\n",
    "    - df: pd.DataFrame\n",
    "        DataFrame contendo a coluna de retornos e metadados / DataFrame with return column and metadata.\n",
    "    - return_col: str\n",
    "        Nome da coluna de retornos logar√≠tmicos / Name of log return column.\n",
    "    - block_height_col: str\n",
    "        Nome da coluna de altura do bloco / Name of block height column.\n",
    "    - block_timestamp_col: str\n",
    "        Nome da coluna de timestamp / Name of timestamp column.\n",
    "    - window: int\n",
    "        Tamanho da janela para c√°lculo da rolling volatility / Rolling window size.\n",
    "\n",
    "    Retorna / Returns:\n",
    "    - pd.DataFrame com colunas: block_height, block_timestamp, volatility_rolling, volatility_zscore, volatility_jump_flag\n",
    "    / DataFrame with volatility features and metadata.\n",
    "    \"\"\"\n",
    "\n",
    "    # ================================\n",
    "    # PREPARA√á√ÉO DA BASE\n",
    "    # ================================\n",
    "    df = df[[return_col, block_height_col, block_timestamp_col]].copy()\n",
    "    # --> Seleciona apenas as colunas necess√°rias / Keep only relevant columns <--\n",
    "\n",
    "    # ================================\n",
    "    # C√ÅLCULO DAS FEATURES DE VOLATILIDADE\n",
    "    # ================================\n",
    "    df[\"volatility_rolling\"] = df[return_col].rolling(window).std()\n",
    "    # --> Desvio padr√£o m√≥vel como proxy de volatilidade / Rolling standard deviation <--\n",
    "\n",
    "    df[\"volatility_zscore\"] = (\n",
    "        df[\"volatility_rolling\"] - df[\"volatility_rolling\"].rolling(window).mean()\n",
    "    ) / df[\"volatility_rolling\"].rolling(window).std()\n",
    "    # --> Z-score da volatilidade em rela√ß√£o √† m√©dia hist√≥rica local / Local z-score of volatility <--\n",
    "\n",
    "    df[\"volatility_jump_flag\"] = (df[\"volatility_zscore\"].abs() > 2).astype(int)\n",
    "    # --> Flag bin√°ria para detectar picos de volatilidade (|z| > 2) / Binary flag for extreme volatility <--\n",
    "\n",
    "    # ================================\n",
    "    # RETORNO FINAL\n",
    "    # ================================\n",
    "    return df[[block_height_col, block_timestamp_col, \"volatility_rolling\", \"volatility_zscore\", \"volatility_jump_flag\"]]\n",
    "    # --> Retorna apenas colunas √∫teis para jun√ß√£o / Return only relevant feature columns <--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_volatility = extract_volatility_features(df_returns)\n",
    "display(df_volatility.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Decomposicao Estrutural da Serie (STL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Modelo Aditivo de S√©ries Temporais\n",
    "\n",
    "A decomposi√ß√£o STL (Seasonal-Trend decomposition using Loess) separa uma s√©rie temporal $ Y_t $ em tr√™s componentes principais:\n",
    "\n",
    "$\n",
    "Y_t = T_t + S_t + R_t\n",
    "$\n",
    "\n",
    "Onde:\n",
    "\n",
    "- $ T_t $: tend√™ncia (trend) ‚Äî representa a varia√ß√£o de longo prazo  \n",
    "- $ S_t $: sazonalidade (seasonal) ‚Äî padr√µes que se repetem em ciclos regulares  \n",
    "- $ R_t $: res√≠duo (residual) ‚Äî ru√≠do ou componente aleat√≥rio\n",
    "\n",
    "---\n",
    "\n",
    "STL ‚Äì *Seasonal and Trend decomposition using Loess*\n",
    "\n",
    "STL √© uma t√©cnica robusta que utiliza regress√£o local (LOESS) para suavizar e estimar cada componente separadamente. Seu diferencial:\n",
    "\n",
    "- Permite **sazonalidade vari√°vel no tempo**\n",
    "- Suporta **dados com outliers**\n",
    "- √â parametriz√°vel com escolha do per√≠odo e robustez\n",
    "\n",
    "---\n",
    "\n",
    "Interpreta√ß√£o pr√°tica:\n",
    "\n",
    "- A **tend√™ncia** $ T_t $ permite identificar a dire√ß√£o geral do fen√¥meno (ex: crescimento do pre√ßo).\n",
    "- A **sazonalidade** $ S_t $ revela padr√µes peri√≥dicos (ex: ciclos mensais ou semanais).\n",
    "- O **res√≠duo** $ R_t $ pode ser analisado para investigar anomalias, choques ou ru√≠do puro.\n",
    "\n",
    "---\n",
    "\n",
    "Aplica√ß√£o:\n",
    "\n",
    "Essa decomposi√ß√£o √© √∫til para:\n",
    "\n",
    "- **Pr√©-processamento** de s√©ries para modelos preditivos\n",
    "- **Detec√ß√£o de anomalias** (com base nos res√≠duos)\n",
    "- **An√°lise explorat√≥ria** de comportamento estrutural da s√©rie\n",
    "\n",
    "---\n",
    "\n",
    "Reversibilidade do modelo:\n",
    "\n",
    "Como se trata de uma decomposi√ß√£o aditiva, podemos sempre reconstruir a s√©rie original somando os componentes:\n",
    "\n",
    "$\n",
    "Y_t = T_t + S_t + R_t\n",
    "$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üî∂ ‚Çø -----> Tend√™ncia, Sazonalidade e Res√≠duo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stl = df_time_index.resample('h').mean()\n",
    "# --> Reamostra os dados com frequ√™ncia hor√°ria, calculando a m√©dia por hora / Resamples data to hourly frequency, taking the mean per hour <--\n",
    "\n",
    "df_stl[\"btc_price_usd\"] = df_stl[\"btc_price_usd\"].interpolate()\n",
    "# --> Interpola valores ausentes no pre√ßo do Bitcoin / Interpolates missing Bitcoin price values <--\n",
    "\n",
    "df_stl = df_stl[df_stl[\"btc_price_usd\"].notna()]\n",
    "# --> Remove qualquer linha restante com valores ausentes / Drops any remaining rows with missing values <--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# APLICA√á√ÉO DA DECOMPOSI√á√ÉO STL / STL DECOMPOSITION APPLICATION\n",
    "# ================================================================\n",
    "def apply_stl_decomposition(\n",
    "    df: pd.DataFrame,\n",
    "    target_col: str,\n",
    "    block_height_col: str = \"block_height\",\n",
    "    block_timestamp_col: str = \"block_timestamp\",\n",
    "    period: int = 48\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aplica a decomposi√ß√£o STL em uma coluna de s√©rie temporal e retorna os componentes.\n",
    "    / Applies STL decomposition to a time series column and returns its components.\n",
    "\n",
    "    Par√¢metros / Parameters:\n",
    "    - df: pd.DataFrame\n",
    "        DataFrame com colunas de pre√ßo, timestamp e altura do bloco /\n",
    "        DataFrame containing price, timestamp and block height.\n",
    "    - target_col: str\n",
    "        Nome da coluna alvo para decomposi√ß√£o / Name of the column to decompose.\n",
    "    - block_height_col: str\n",
    "        Nome da coluna de altura do bloco / Name of the block height column.\n",
    "    - block_timestamp_col: str\n",
    "        Nome da coluna de timestamp / Name of the timestamp column.\n",
    "    - period: int\n",
    "        Periodicidade da s√©rie para sazonalidade / Seasonality period.\n",
    "\n",
    "    Retorna / Returns:\n",
    "    - pd.DataFrame com colunas: block_height, block_timestamp, trend, seasonal, resid /\n",
    "      DataFrame with STL components and metadata.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ================================\n",
    "    # PREPARA√á√ÉO DA S√âRIE\n",
    "    # ================================\n",
    "    df = df[[target_col, block_height_col, block_timestamp_col]].dropna().copy()\n",
    "    # --> Seleciona e limpa colunas necess√°rias / Select and clean necessary columns <--\n",
    "\n",
    "    stl = STL(df[target_col], period=period, robust=True)\n",
    "    # --> Configura a decomposi√ß√£o STL / Configures STL decomposition <--\n",
    "\n",
    "    result = stl.fit()\n",
    "    # --> Executa o ajuste da decomposi√ß√£o / Fits the STL decomposition <--\n",
    "\n",
    "    df_stl = pd.DataFrame({\n",
    "        block_height_col: df[block_height_col].values,\n",
    "        block_timestamp_col: df[block_timestamp_col].values,\n",
    "        \"trend\": result.trend,\n",
    "        \"seasonal\": result.seasonal,\n",
    "        \"resid\": result.resid\n",
    "    }, index=df.index)\n",
    "    # --> Cria DataFrame com componentes STL e metadados / Creates DataFrame with STL components <--\n",
    "\n",
    "    return df_stl\n",
    "    # --> Retorna apenas colunas √∫teis para merge / Returns aligned components with metadata <--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S√©rie de pre√ßo limpa e com √≠ndice reiniciado\n",
    "btc_price_clean = df_stl[\"btc_price_usd\"].dropna().reset_index(drop=True)\n",
    "\n",
    "# Aplica a decomposi√ß√£o STL com essa s√©rie\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "result = STL(btc_price_clean, period=48, robust=True).fit()\n",
    "\n",
    "# Monta o DataFrame decomposto com mesmo √≠ndice\n",
    "df_stl_dec = pd.DataFrame({\n",
    "    \"trend\": result.trend,\n",
    "    \"seasonal\": result.seasonal,\n",
    "    \"resid\": result.resid\n",
    "}, index=btc_price_clean.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üî∂ ‚Çø -----> Tend√™ncia, Sazonalidade e Res√≠duo - An√°lise Gr√°fica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stl_decomposition(\n",
    "    df_stl=df_stl_dec,\n",
    "    price_series=df_stl[\"btc_price_usd\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üî∂ ‚Çø -----> Extra√ß√£o de Features Estruturais (spikiness, curvature, linearity, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stl = df_stl.reset_index()\n",
    "\n",
    "# Define a janela da s√©rie (por exemplo, 2 dias)\n",
    "window = df_stl[\"btc_price_usd\"].iloc[-96:]  # 48 * 2 per√≠odos de 30 minutos\n",
    "\n",
    "features_stl = extract_stl_features(\n",
    "    series=window,\n",
    "    block_height=df_stl[\"block_height\"].iloc[-1],\n",
    "    block_timestamp=df_stl[\"block_timestamp\"].iloc[-1],\n",
    "    period=5\n",
    ")\n",
    "\n",
    "display(features_stl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üî∂ ‚Çø -----> An√°lise dos Componentes em Subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para df_seasonal\n",
    "df_seasonal = df_features_temp.toPandas()  # Se precisar de ordena√ß√£o aqui, pode usar:\n",
    "df_seasonal = df_seasonal.sort_values(\"block_timestamp\")  # Ordena pelo timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_seasonal_daily_line(df_seasonal, timestamp_col=\"block_timestamp\", price_col=\"btc_price_usd\", ano_alvo=2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_seasonal_weekly_line(df_seasonal, timestamp_col=\"block_timestamp\", price_col=\"btc_price_usd\", ano_alvo=2025, image_path=image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_weekly_seasonality_all_years(df_seasonal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_intraday_price_by_hour(\n",
    "    df=df_seasonal,\n",
    "    year_filter=2025,\n",
    "    image_path=\"/Users/rodrigocampos/Documents/Bitcoin/project/src/visualizations/BTC_black.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo:\n",
    "plot_bitcoin_seasonal_patterns(df_seasonal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üî∂ ‚Çø -----> Teste Estat√≠stico de Intera√ß√£o via ANOVA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "ANOVA de Duas Vias com Intera√ß√£o\n",
    "\n",
    "A ANOVA (An√°lise de Vari√¢ncia) testa se **as m√©dias de um grupo s√£o estatisticamente diferentes** entre si. No caso de **duas vari√°veis categ√≥ricas**, podemos incluir um **termo de intera√ß√£o** para avaliar se o efeito de uma vari√°vel depende da outra.\n",
    "\n",
    "---\n",
    "\n",
    "Modelo de regress√£o com intera√ß√£o:\n",
    "\n",
    "Seja a vari√°vel resposta $ Y $ (neste caso, o pre√ßo do Bitcoin) e duas vari√°veis categ√≥ricas: m√™s ($ M $) e ano ($ A $). O modelo com intera√ß√£o √© dado por:\n",
    "\n",
    "$\n",
    "Y = \\mu + \\alpha_M + \\beta_A + (\\alpha\\beta)_{M,A} + \\epsilon\n",
    "$\n",
    "\n",
    "Onde:\n",
    "\n",
    "- $ \\mu $: m√©dia geral\n",
    "- $ \\alpha_M $: efeito do m√™s\n",
    "- $ \\beta_A $: efeito do ano\n",
    "- $ (\\alpha\\beta)_{M,A} $: efeito de intera√ß√£o entre m√™s e ano\n",
    "- $ \\epsilon $: erro aleat√≥rio\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar colunas necess√°rias\n",
    "df_seasonal[\"block_timestamp\"] = pd.to_datetime(df_seasonal[\"block_timestamp\"])\n",
    "df_seasonal[\"Ano\"] = df_seasonal[\"block_timestamp\"].dt.year.astype(str)  # precisa ser string para categ√≥rico\n",
    "df_seasonal[\"Mes\"] = df_seasonal[\"block_timestamp\"].dt.month_name()\n",
    "\n",
    "# ==========================\n",
    "# Modelo com Intera√ß√£o\n",
    "# ==========================\n",
    "modelo_interacao = smf.ols(\"btc_price_usd ~ C(Mes) * C(Ano)\", data=df_seasonal).fit()\n",
    "\n",
    "# ==========================\n",
    "# ANOVA para testar signific√¢ncia da intera√ß√£o\n",
    "# ==========================\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "anova_resultado = anova_lm(modelo_interacao)\n",
    "\n",
    "# Visualizar resultado\n",
    "print(anova_resultado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "\n",
    "# ================================================================\n",
    "# EXTRA√á√ÉO DE FEATURES SAZONAIS A PARTIR DE M√äS E ANO\n",
    "# EXTRACTION OF SEASONAL FEATURES BASED ON MONTH AND YEAR\n",
    "# ================================================================\n",
    "def extract_seasonal_anova_features(\n",
    "    df: pd.DataFrame,\n",
    "    date_col: str = \"block_timestamp\",\n",
    "    target_col: str = \"btc_price_usd\",\n",
    "    block_height: int = None,\n",
    "    block_timestamp: pd.Timestamp = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Gera vari√°veis categ√≥ricas de M√™s e Ano e realiza ANOVA com intera√ß√£o para identificar sazonalidade.\n",
    "    / Generates categorical Month-Year variables and performs ANOVA to detect seasonality interactions.\n",
    "    \n",
    "    Par√¢metros / Parameters:\n",
    "    - df: pd.DataFrame\n",
    "        DataFrame contendo a s√©rie temporal / DataFrame containing time series.\n",
    "    - date_col: str\n",
    "        Nome da coluna de data / Name of timestamp column.\n",
    "    - target_col: str\n",
    "        Coluna de valores num√©ricos alvo / Column with target values.\n",
    "    - block_height: int\n",
    "        Altura do bloco de refer√™ncia / Reference block height.\n",
    "    - block_timestamp: pd.Timestamp\n",
    "        Timestamp de refer√™ncia do bloco / Reference block timestamp.\n",
    "\n",
    "    Retorna / Returns:\n",
    "    - pd.DataFrame com colunas: block_height, block_timestamp, anova_p_mes, anova_p_ano, anova_p_interacao /\n",
    "      DataFrame with tracking columns and ANOVA p-values for month, year, and interaction.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # ================================\n",
    "    # EXTRA√á√ÉO DE M√äS E ANO / EXTRACT MONTH & YEAR\n",
    "    # ================================\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    # --> Garante tipo datetime / Ensures datetime format <--\n",
    "\n",
    "    df[\"Ano\"] = df[date_col].dt.year.astype(str)\n",
    "    df[\"Mes\"] = df[date_col].dt.month_name()\n",
    "    # --> Extrai componentes temporais para ANOVA / Extracts month and year for ANOVA <--\n",
    "\n",
    "    # ================================\n",
    "    # AJUSTE DO MODELO COM INTERA√á√ÉO / FIT INTERACTION MODEL\n",
    "    # ================================\n",
    "    formula = f\"{target_col} ~ C(Mes) * C(Ano)\"\n",
    "    modelo = smf.ols(formula=formula, data=df).fit()\n",
    "    # --> Ajusta modelo com intera√ß√£o m√™s x ano / Fit model with month-year interaction <--\n",
    "\n",
    "    # ================================\n",
    "    # ANOVA E P-VALUES / ANOVA AND P-VALUES\n",
    "    # ================================\n",
    "    anova_result = anova_lm(modelo)\n",
    "\n",
    "    p_mes = anova_result.loc[\"C(Mes)\", \"PR(>F)\"] if \"C(Mes)\" in anova_result.index else None\n",
    "    p_ano = anova_result.loc[\"C(Ano)\", \"PR(>F)\"] if \"C(Ano)\" in anova_result.index else None\n",
    "    p_inter = anova_result.loc[\"C(Mes):C(Ano)\", \"PR(>F)\"] if \"C(Mes):C(Ano)\" in anova_result.index else None\n",
    "    # --> P-valores de cada fator e da intera√ß√£o / P-values for month, year and interaction <--\n",
    "\n",
    "    # ================================\n",
    "    # RETORNO DAS FEATURES / FEATURE OUTPUT\n",
    "    # ================================\n",
    "    return pd.DataFrame([{\n",
    "        \"block_height\": block_height,\n",
    "        \"block_timestamp\": block_timestamp,\n",
    "        \"anova_p_mes\": p_mes,\n",
    "        \"anova_p_ano\": p_ano,\n",
    "        \"anova_p_interacao\": p_inter\n",
    "    }])\n",
    "    # --> Retorna como linha √∫nica com chaves de rastreio / Returns row with tracking keys <--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_seasonality = extract_seasonal_anova_features(df_seasonal)\n",
    "display(df_seasonality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Analise Espectral (FFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para df_fft\n",
    "df_fft = df_time_index.resample('d').mean()\n",
    "df_fft = df_fft.sort_index()  # Garantir que est√° ordenado pelo √≠ndice (block_timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üî∂ ‚Çø -----> Frequ√™ncia Dominante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Transformada R√°pida de Fourier (FFT)\n",
    "\n",
    "Dada uma s√©rie temporal discreta $ x[n] $ de tamanho $ N $, sua transformada discreta de Fourier √© definida por:\n",
    "\n",
    "$\n",
    "X[k] = \\sum_{n=0}^{N-1} x[n] \\cdot e^{-j2\\pi kn/N}\n",
    "$\n",
    "\n",
    "Onde:\n",
    "\n",
    "- $ X[k] $: componente de frequ√™ncia no √≠ndice $( k $)\n",
    "- $ N $: n√∫mero total de pontos na s√©rie\n",
    "- $ j $: unidade imagin√°ria $( j^2 = -1 )$\n",
    "\n",
    "A frequ√™ncia correspondente a cada √≠ndice \\( k \\) √© dada por:\n",
    "\n",
    "$\n",
    "f[k] = \\frac{k}{N}\n",
    "$\n",
    "\n",
    "Para obter as **frequ√™ncias positivas** e suas **amplitudes** (magnitudes):\n",
    "\n",
    "- Frequ√™ncias positivas:  \n",
    "  $\n",
    "  f = \\text{fft\\_freqs}[f > 0]\n",
    "  $\n",
    "\n",
    "- Magnitudes associadas:  \n",
    "  $\n",
    "  |X[k]| = \\sqrt{\\Re(X[k])^2 + \\Im(X[k])^2}\n",
    "  $\n",
    "\n",
    "---\n",
    "\n",
    "Identifica√ß√£o da Frequ√™ncia Dominante\n",
    "\n",
    "A **frequ√™ncia dominante** √© aquela cujo valor de \\( |X[k]| \\) (amplitude) √© o **m√°ximo** dentre todas as componentes positivas:\n",
    "\n",
    "$\n",
    "f_{\\text{dom}} = f[argmax(|X[k]|)]\n",
    "$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# PREPARA√á√ÉO DA S√âRIE PARA FFT / PREPARE SERIES FOR FFT\n",
    "# ================================================================\n",
    "def prepare_fft_data(series: pd.Series):\n",
    "    \"\"\"\n",
    "    Pr√©-processa a s√©rie e retorna as frequ√™ncias e amplitudes positivas.\n",
    "    / Preprocesses the series and returns positive frequencies and magnitudes.\n",
    "    \"\"\"\n",
    "\n",
    "    series = series.dropna().values\n",
    "    # --> Remove valores ausentes e converte para array NumPy / Drops missing values and converts to NumPy array <--\n",
    "\n",
    "    n = len(series)\n",
    "    # --> Define o comprimento da s√©rie para c√°lculo da FFT / Defines series length for FFT calculation <--\n",
    "\n",
    "    fft_vals = np.fft.fft(series)\n",
    "    # --> Calcula os coeficientes da Transformada de Fourier / Computes the Fourier transform coefficients <--\n",
    "\n",
    "    fft_freqs = np.fft.fftfreq(n)\n",
    "    # --> Gera a grade de frequ√™ncias correspondente / Generates the corresponding frequency grid <--\n",
    "\n",
    "    pos_mask = fft_freqs > 0\n",
    "    # --> Cria uma m√°scara para manter apenas frequ√™ncias positivas / Creates mask to retain only positive frequencies <--\n",
    "\n",
    "    freqs = fft_freqs[pos_mask]\n",
    "    # --> Frequ√™ncias positivas / Positive frequencies <--\n",
    "\n",
    "    magnitudes = np.abs(fft_vals[pos_mask])\n",
    "    # --> Magnitudes (amplitudes absolutas) das componentes positivas / Absolute magnitudes of positive components <--\n",
    "\n",
    "    return freqs, magnitudes\n",
    "    # --> Retorna as frequ√™ncias e magnitudes para an√°lise espectral / Returns frequencies and magnitudes for spectral analysis <--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs, magnitudes = prepare_fft_data(df_fft[\"btc_price_usd\"])\n",
    "\n",
    "print(\"Frequ√™ncias detectadas:\", freqs[:5])\n",
    "print(\"Magnitudes:\", magnitudes[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üî∂ ‚Çø -----> Energia Espectral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Energia Espectral e Raz√£o de Energia\n",
    "\n",
    "Seja uma s√©rie temporal transformada via FFT, com magnitudes espectrais $ M_k $, a **energia espectral total** da s√©rie √© dada por:\n",
    "\n",
    "$\n",
    "E_{\\text{total}} = \\sum_{k=1}^{N} M_k^2\n",
    "$\n",
    "\n",
    "Onde:\n",
    "\n",
    "- $ M_k $: magnitude da componente de frequ√™ncia $ k $\n",
    "- $ N $: n√∫mero total de componentes de frequ√™ncia\n",
    "\n",
    "A **energia top‚ÄëK** corresponde √† soma das $ K $ maiores contribui√ß√µes de energia:\n",
    "\n",
    "$\n",
    "E_{\\text{top‚ÄëK}} = \\sum_{k \\in \\text{Top‚ÄëK}} M_k^2\n",
    "$\n",
    "\n",
    "A **raz√£o de energia espectral** √© definida como:\n",
    "\n",
    "$\n",
    "\\text{Raz√£o} = \\frac{E_{\\text{top‚ÄëK}}}{E_{\\text{total}}}\n",
    "$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# C√ÅLCULO DA RAZ√ÉO DE ENERGIA / ENERGY RATIO\n",
    "# ================================================================\n",
    "def calculate_energy_ratio(magnitudes: np.ndarray, top_k: int):\n",
    "    \"\"\"\n",
    "    Calcula a raz√£o entre a energia das top-k amplitudes e a energia total.\n",
    "    / Calculates the ratio between top-k dominant energy and total energy.\n",
    "    \"\"\"\n",
    "\n",
    "    total_energy = np.sum(magnitudes**2)\n",
    "    # --> Calcula a energia total da s√©rie (soma dos quadrados das magnitudes) / Computes total energy (sum of squared magnitudes) <--\n",
    "\n",
    "    topk_energy = np.sum(np.sort(magnitudes**2)[-top_k:])\n",
    "    # --> Soma da energia das top-k frequ√™ncias dominantes / Sum of energy from top-k dominant frequencies <--\n",
    "\n",
    "    return topk_energy / total_energy\n",
    "    # --> Retorna a raz√£o de energia concentrada nas top-k componentes / Returns energy ratio of dominant frequencies <--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_ratio = calculate_energy_ratio(magnitudes, top_k=3)\n",
    "print(\"Raz√£o de energia espectral:\", energy_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üî∂ ‚Çø -----> Entropia Espectral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Entropia Espectral (Spectral Entropy)\n",
    "\n",
    "A **entropia espectral** quantifica o grau de desordem ou dispers√£o da energia em diferentes frequ√™ncias de uma s√©rie temporal. Ela √© baseada na distribui√ß√£o normalizada das magnitudes do espectro de Fourier.\n",
    "\n",
    "---\n",
    "\n",
    "Defini√ß√£o formal:\n",
    "\n",
    "Seja $ M_k $ a magnitude da componente de frequ√™ncia $ k $, a **distribui√ß√£o de probabilidade espectral** $ P_k $ √© definida como:\n",
    "\n",
    "$\n",
    "P_k = \\frac{M_k}{\\sum_{i=1}^{N} M_i}\n",
    "$\n",
    "\n",
    "A entropia espectral √© ent√£o dada por:\n",
    "\n",
    "$\n",
    "H = - \\sum_{k=1}^{N} P_k \\cdot \\log_2(P_k)\n",
    "$\n",
    "\n",
    "> Obs: Na pr√°tica, adiciona-se um termo $ \\varepsilon $ muito pequeno para evitar log de zero:  \n",
    "$\n",
    "H = - \\sum_{k=1}^{N} P_k \\cdot \\log_2(P_k + \\varepsilon)\n",
    "\\quad \\text{com } \\varepsilon = 10^{-12}\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "Interpreta√ß√£o:\n",
    "\n",
    "- Baixa entropia espectral: concentra√ß√£o de energia em poucas frequ√™ncias ‚Üí sinal mais previs√≠vel ou peri√≥dico.\n",
    "- Alta entropia espectral: energia distribu√≠da em muitas frequ√™ncias ‚Üí sinal mais complexo ou ru√≠do branco.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# C√ÅLCULO DA ENTROPIA ESPECTRAL / SPECTRAL ENTROPY\n",
    "# ================================================================\n",
    "def calculate_spectral_entropy(magnitudes: np.ndarray):\n",
    "    \"\"\"\n",
    "    Calcula a entropia espectral da distribui√ß√£o de frequ√™ncia.\n",
    "    / Computes the spectral entropy of the frequency distribution.\n",
    "    \"\"\"\n",
    "\n",
    "    prob_dist = magnitudes / np.sum(magnitudes)\n",
    "    # --> Converte magnitudes em distribui√ß√£o de probabilidade / Converts magnitudes into a probability distribution <--\n",
    "\n",
    "    return -np.sum(prob_dist * np.log2(prob_dist + 1e-12))\n",
    "    # --> Aplica a f√≥rmula da entropia de Shannon / Applies Shannon entropy formula <--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# FUN√á√ÉO PRINCIPAL / MAIN FEATURE EXTRACTION FUNCTION\n",
    "# ================================================================\n",
    "def extract_fft_features(series: pd.Series, top_k: int = 3) -> dict:\n",
    "    \"\"\"\n",
    "    Extrai features estruturais da s√©rie temporal via FFT.\n",
    "    / Extracts structural features from time series using FFT.\n",
    "    \"\"\"\n",
    "\n",
    "    freqs, magnitudes = prepare_fft_data(series)\n",
    "    # --> Pr√©-processa a s√©rie e obt√©m frequ√™ncias e magnitudes positivas / Preprocesses the series and gets positive frequencies and magnitudes <--\n",
    "\n",
    "    return {\n",
    "        \"fft_dominant_freq\": freqs[np.argmax(magnitudes)],\n",
    "        # --> Frequ√™ncia com maior magnitude (componente dominante) / Frequency with highest magnitude (dominant component) <--\n",
    "\n",
    "        \"fft_energy_ratio\": calculate_energy_ratio(magnitudes, top_k),\n",
    "        # --> Raz√£o de energia concentrada nas top-k componentes / Energy ratio from top-k dominant components <--\n",
    "\n",
    "        \"fft_peak_amplitude\": np.max(magnitudes),\n",
    "        # --> Valor da amplitude de pico entre as frequ√™ncias / Peak amplitude value among all frequencies <--\n",
    "\n",
    "        \"fft_spectral_entropy\": calculate_spectral_entropy(magnitudes)\n",
    "        # --> Entropia espectral (grau de desorganiza√ß√£o da energia) / Spectral entropy (energy dispersion measure) <--\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy = calculate_spectral_entropy(magnitudes)\n",
    "print(\"Entropia espectral:\", entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üî∂ ‚Çø -----> Resultado Geral das Frequ√™ncias Dominantes e Entropia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_fft = extract_fft_features(df_fft[\"btc_price_usd\"], top_k=3)\n",
    "\n",
    "for k, v in features_fft.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "    \n",
    "period_in_steps = 1 / features_fft[\"fft_dominant_freq\"]\n",
    "print(f\" Ciclo dominante estimado: {period_in_steps:.2f} passos\")\n",
    "\n",
    "features_fft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üî∂ ‚Çø -----> Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================================================================\n",
    "# PR√â-PROCESSAMENTO PARA FFT / PREPROCESSING FOR FFT\n",
    "# ================================================================\n",
    "def prepare_fft_data(series: pd.Series):\n",
    "    \"\"\"\n",
    "    Pr√©-processa a s√©rie e retorna frequ√™ncias e magnitudes positivas.\n",
    "    / Preprocesses the series and returns positive frequencies and magnitudes.\n",
    "    \"\"\"\n",
    "    series = series.dropna().values\n",
    "    n = len(series)\n",
    "    fft_vals = np.fft.fft(series)\n",
    "    fft_freqs = np.fft.fftfreq(n)\n",
    "\n",
    "    pos_mask = fft_freqs > 0\n",
    "    # --> Considera apenas frequ√™ncias positivas / Keep only positive frequencies <--\n",
    "\n",
    "    freqs = fft_freqs[pos_mask]\n",
    "    magnitudes = np.abs(fft_vals[pos_mask])\n",
    "    # --> Calcula m√≥dulo das componentes espectrais / Compute magnitude of spectral components <--\n",
    "\n",
    "    return freqs, magnitudes\n",
    "\n",
    "# ================================================================\n",
    "# C√ÅLCULO DA RAZ√ÉO DE ENERGIA / ENERGY RATIO\n",
    "# ================================================================\n",
    "def calculate_energy_ratio(magnitudes: np.ndarray, top_k: int) -> float:\n",
    "    \"\"\"\n",
    "    Calcula a raz√£o entre a energia das top-k amplitudes e a energia total.\n",
    "    / Calculates the ratio between top-k dominant energy and total energy.\n",
    "    \"\"\"\n",
    "    total_energy = np.sum(magnitudes**2)\n",
    "    # --> Energia total = soma dos quadrados das magnitudes / Total energy = sum of squared magnitudes <--\n",
    "\n",
    "    topk_energy = np.sum(np.sort(magnitudes**2)[-top_k:])\n",
    "    # --> Energia nas k maiores componentes / Energy from top-k dominant components <--\n",
    "\n",
    "    return topk_energy / total_energy\n",
    "    # --> Retorna a raz√£o de concentra√ß√£o energ√©tica / Returns energy concentration ratio <--\n",
    "\n",
    "# ================================================================\n",
    "# C√ÅLCULO DA ENTROPIA ESPECTRAL / SPECTRAL ENTROPY\n",
    "# ================================================================\n",
    "def calculate_spectral_entropy(magnitudes: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calcula a entropia espectral da distribui√ß√£o de frequ√™ncia.\n",
    "    / Computes the spectral entropy of the frequency distribution.\n",
    "    \"\"\"\n",
    "    prob_dist = magnitudes / np.sum(magnitudes)\n",
    "    # --> Distribui√ß√£o de probabilidade normalizada / Normalized probability distribution <--\n",
    "\n",
    "    return -np.sum(prob_dist * np.log2(prob_dist + 1e-12))\n",
    "    # --> Entropia de Shannon (com estabilidade num√©rica) / Shannon entropy (with numerical stability) <--\n",
    "\n",
    "# ================================================================\n",
    "# EXTRA√á√ÉO DE FEATURES FFT COM JANELAS M√ìVEIS\n",
    "# ================================================================\n",
    "def extract_fft_features_df(\n",
    "    series: pd.Series,\n",
    "    window_size: int = 60,\n",
    "    step: int = 10,\n",
    "    top_k: int = 3\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extrai features estruturais da s√©rie via FFT e retorna um DataFrame com m√∫ltiplas janelas m√≥veis.\n",
    "    / Extracts structural features from time series using FFT and returns a DataFrame with multiple rolling windows.\n",
    "\n",
    "    Par√¢metros / Parameters:\n",
    "    - series: pd.Series\n",
    "        S√©rie temporal com √≠ndice temporal / Time-indexed numeric series.\n",
    "    - window_size: int\n",
    "        Tamanho da janela m√≥vel (n√∫mero de pontos) / Size of the rolling window (number of points).\n",
    "    - step: int\n",
    "        Passo entre janelas consecutivas / Step size between consecutive windows.\n",
    "    - top_k: int\n",
    "        N√∫mero de componentes principais de frequ√™ncia / Number of dominant frequency components.\n",
    "\n",
    "    Retorna / Returns:\n",
    "    - pd.DataFrame com features para todas as janelas m√≥veis / DataFrame with features for all rolling windows.\n",
    "    \"\"\"\n",
    "    result_list = []\n",
    "\n",
    "    for i in range(0, len(series) - window_size + 1, step):\n",
    "        window = series.iloc[i:i + window_size]\n",
    "\n",
    "        # Dados de rastreio\n",
    "        block_height = series.index[i + window_size - 1]\n",
    "        block_timestamp = series.index[i + window_size - 1]\n",
    "\n",
    "        # Processamento de FFT\n",
    "        freqs, magnitudes = prepare_fft_data(window)\n",
    "\n",
    "        # Extra√ß√£o das features\n",
    "        features = {\n",
    "            \"block_height\": block_height,\n",
    "            \"block_timestamp\": block_timestamp,\n",
    "            \"fft_dominant_freq\": freqs[np.argmax(magnitudes)],\n",
    "            # --> Frequ√™ncia dominante (maior pico) / Dominant frequency (highest peak) <--\n",
    "\n",
    "            \"fft_energy_ratio\": calculate_energy_ratio(magnitudes, top_k),\n",
    "            # --> Raz√£o da energia nas top-k frequ√™ncias / Energy ratio in top-k components <--\n",
    "\n",
    "            \"fft_peak_amplitude\": np.max(magnitudes),\n",
    "            # --> Amplitude m√°xima / Maximum amplitude <--\n",
    "\n",
    "            \"fft_spectral_entropy\": calculate_spectral_entropy(magnitudes)\n",
    "            # --> Entropia espectral / Spectral entropy <--\n",
    "        }\n",
    "\n",
    "        # Adiciona os resultados para cada janela\n",
    "        result_list.append(features)\n",
    "\n",
    "    # Retorna todos os resultados concatenados em um DataFrame\n",
    "    return pd.DataFrame(result_list)\n",
    "    # --> Retorna as features para todas as janelas m√≥veis / Returns features for all rolling windows <--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo par√¢metros de janela e passo\n",
    "window_size = 60  # Tamanho da janela (exemplo: 60 per√≠odos de dados)\n",
    "step = 10         # Passo de 10 per√≠odos entre as janelas\n",
    "\n",
    "# Chamando a fun√ß√£o para extrair as features FFT para a s√©rie temporal\n",
    "fft_features_df = extract_fft_features_df(series=df_fft['btc_price_usd'], window_size=window_size, step=step)\n",
    "\n",
    "# Exibindo as primeiras 5 linhas do DataFrame resultante\n",
    "display(fft_features_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_peaks = extract_peak_features(df_fft, column=\"btc_price_usd\", distance=8)\n",
    "display(df_peaks.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Janela de exemplo\n",
    "window = df_fft[\"btc_price_usd\"].iloc[-60:]\n",
    "block_height_final = df_fft[\"block_height\"].iloc[-1]\n",
    "timestamp_final = df_fft[\"block_timestamp\"].iloc[-1]\n",
    "\n",
    "# Execu√ß√£o\n",
    "df_ciclico = extract_cyclic_features(\n",
    "    series=window,\n",
    "    block_height=block_height_final,\n",
    "    block_timestamp=timestamp_final,\n",
    "    distance=8\n",
    ")\n",
    "\n",
    "display(df_ciclico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = df_fft[\"btc_price_usd\"].dropna()\n",
    "# --> Remove valores ausentes da s√©rie / Drops missing values from the series <--\n",
    "\n",
    "n = len(series)\n",
    "# --> N√∫mero total de observa√ß√µes na s√©rie / Total number of observations in the series <--\n",
    "\n",
    "fft_vals = np.fft.fft(series)\n",
    "# --> Aplica a Transformada R√°pida de Fourier (FFT) / Applies Fast Fourier Transform (FFT) <--\n",
    "\n",
    "fft_freqs = np.fft.fftfreq(n)\n",
    "# --> Frequ√™ncias associadas aos coeficientes FFT / Frequencies corresponding to FFT coefficients <--\n",
    "\n",
    "# ===========================\n",
    "# FILTRO: FREQU√äNCIAS POSITIVAS\n",
    "# ===========================\n",
    "\n",
    "mask = fft_freqs > 0\n",
    "fft_freqs = fft_freqs[mask]\n",
    "fft_vals = fft_vals[mask]\n",
    "# --> Mant√©m apenas as frequ√™ncias positivas (metade direita do espectro) / Keeps only positive frequencies (right half of the spectrum) <--\n",
    "\n",
    "# ================================\n",
    "# C√ÅLCULO DA POT√äNCIA / POWER SPECTRUM\n",
    "# ================================\n",
    "\n",
    "power = np.abs(fft_vals)**2\n",
    "# --> Pot√™ncia espectral (amplitude¬≤) de cada componente harm√¥nico / Spectral power (amplitude¬≤) for each harmonic <--\n",
    "\n",
    "periods = 1 / fft_freqs\n",
    "# --> Converte frequ√™ncia em per√≠odo (em unidades de tempo) / Converts frequency to period (in time units) <--\n",
    "\n",
    "# ===================================\n",
    "# FILTRAGEM DE PER√çODOS EXTREMOS\n",
    "# ===================================\n",
    "\n",
    "valid = (periods < 500)\n",
    "periods_filtered = periods[valid]\n",
    "power_filtered = power[valid]\n",
    "# --> Remove per√≠odos muito longos que n√£o s√£o relevantes (ex: acima de 500 unidades) / Filters out long periods (e.g., over 500 units) <--\n",
    "\n",
    "# ===================================\n",
    "# SELE√á√ÉO DOS PRINCIPAIS PONTOS\n",
    "# ===================================\n",
    "\n",
    "top_peaks = np.argsort(power_filtered)[-3:]\n",
    "# --> Seleciona os √≠ndices das 3 maiores pot√™ncias (frequ√™ncias dominantes) / Selects indices of the top 3 dominant frequencies <--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fft_spectrum(\n",
    "    periods=periods_filtered,\n",
    "    power=power_filtered,\n",
    "    fft_vals=fft_vals,\n",
    "    n=n,\n",
    "    top_peaks_idx=top_peaks,\n",
    "    image_path=\"/Users/rodrigocampos/Documents/Bitcoin/project/src/visualizations/BTC_black.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üî∂ ‚Çø -----> Reconstru√ß√£o C√≠clica com FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed = reconstruct_fft(df_fft[\"btc_price_usd\"], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S√©rie de pre√ßos original\n",
    "price_series = df_stl[\"btc_price_usd\"]\n",
    "# --> S√©rie original de pre√ßos / Original price series <--\n",
    "\n",
    "# S√©rie centralizada (remo√ß√£o da tend√™ncia via STL ou suaviza√ß√£o)\n",
    "price_detrended = price_series - df_stl_dec[\"trend\"]\n",
    "# --> Remove a tend√™ncia da s√©rie para aplicar a FFT / Removes trend to isolate cyclical components <--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# PLOTAGEM DA RECONSTRU√á√ÉO FFT\n",
    "# ==========================\n",
    "\n",
    "fig_fft_recon = go.Figure()\n",
    "\n",
    "# --- S√©rie Reconstru√≠da com as principais frequ√™ncias (FFT) ---\n",
    "fig_fft_recon.add_trace(go.Scatter(\n",
    "    x=df_fft.index,\n",
    "    y=reconstructed,\n",
    "    name=\"Reconstru√ß√£o FFT (Top Frequ√™ncias)\",\n",
    "    line=dict(color=\"white\", width=2, dash=\"dot\"),\n",
    "    fill=\"tozeroy\",\n",
    "    fillcolor=\"rgba(229,165,0,0.25)\"\n",
    "))\n",
    "# --> Linha pontilhada branca representando os ciclos reconstru√≠dos via FFT / Dashed white line: cycles reconstructed using FFT <--\n",
    "\n",
    "# --- S√©rie original centralizada (sem tend√™ncia) ---\n",
    "fig_fft_recon.add_trace(go.Scatter(\n",
    "    x=df_fft.index,\n",
    "    y=price_detrended,\n",
    "    name=\"S√©rie Centralizada\",\n",
    "    line=dict(color=\"#E57C1F\", width=1.2),\n",
    "    opacity=0.3\n",
    "))\n",
    "# --> S√©rie de pre√ßos original com tend√™ncia removida (centralizada) / Original series with trend removed (centered) <--\n",
    "\n",
    "# ==========================\n",
    "# LAYOUT DO GR√ÅFICO\n",
    "# ==========================\n",
    "\n",
    "fig_fft_recon.update_layout(\n",
    "    title=\"<b><span style='font-size:22px;'>Reconstru√ß√£o C√≠clica com FFT</span><br><span style='font-size:14px;'>Com base nas principais frequ√™ncias da s√©rie</span></b>\",\n",
    "    xaxis_title=\"Data\",\n",
    "    yaxis_title=\"Amplitude (Centralizada)\",\n",
    "    template=\"plotly_dark\",\n",
    "    height=500,\n",
    "    showlegend=True\n",
    ")\n",
    "# --> T√≠tulo, eixos e estilo escuro com preenchimento harm√¥nico / Title, axis, and dark theme with FFT reconstruction fill <--\n",
    "\n",
    "fig_fft_recon.show()\n",
    "# --> Exibe o gr√°fico final interativo / Displays the final interactive chart <--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Detec√ß√£o de Padr√µes C√≠clicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ================================================================\n",
    "# DETEC√á√ÉO DE PADR√ïES C√çCLICOS / CYCLE PATTERN DETECTION\n",
    "# ================================================================\n",
    "\n",
    "peaks, _ = find_peaks(series, distance=10)\n",
    "# --> Encontra os √≠ndices dos picos com dist√¢ncia m√≠nima entre eles / Finds peak indices with a minimum spacing <--\n",
    "\n",
    "vales, _ = find_peaks(-series, distance=10)\n",
    "# --> Inverte o sinal da s√©rie para detectar m√≠nimos como picos / Inverts signal to find valleys as peaks <--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ciclo_duracoes = np.diff(vales)\n",
    "amplitudes = series[peaks] - series[vales[:len(peaks)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5 Estrutura e Persist√™ncia Temporal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coeficiente de Hurst (Mem√≥ria Longa)\n",
    "\n",
    "O **coeficiente de Hurst** $ H $ √© uma medida de **persist√™ncia de longo prazo** de uma s√©rie temporal. Ele indica se a s√©rie tende a:\n",
    "\n",
    "- Reverter √† m√©dia: $ H < 0.5 $\n",
    "- Ser aleat√≥ria (ru√≠do branco): $ H \\approx 0.5 $\n",
    "- Persistir na dire√ß√£o atual: $ H > 0.5 $\n",
    "\n",
    "O c√°lculo baseia-se no crescimento da vari√¢ncia com o tempo:\n",
    "\n",
    "$\n",
    "\\text{Var}(X_t) \\propto t^{2H}\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "N√∫mero de Cruzamentos com a Mediana\n",
    "\n",
    "Esta m√©trica indica **quantas vezes a s√©rie cruza sua mediana**. Um n√∫mero alto sugere comportamento mais **oscilat√≥rio**, enquanto um n√∫mero baixo sugere **persist√™ncia ou tend√™ncia**.\n",
    "\n",
    "$\n",
    "\\text{crossings} = \\sum_{t=2}^{n} \\mathbf{1}\\{(x_{t-1} - \\text{mediana}) \\cdot (x_t - \\text{mediana}) < 0\\}\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "Maior Regi√£o de Estagna√ß√£o\n",
    "\n",
    "Mede o maior segmento cont√≠nuo da s√©rie com **baixa varia√ß√£o**, representando **zonas de lateraliza√ß√£o** ou aus√™ncia de movimento relevante.\n",
    "\n",
    "$\n",
    "\\text{flat\\_spot} = \\max\\{\\text{dura√ß√£o de regi√µes quase constantes}\\}\n",
    "$\n",
    "\n",
    "Esses tr√™s recursos ajudam a quantificar o grau de **regularidade, mem√≥ria e oscila√ß√£o** da s√©rie, podendo ser √∫teis para detectar **comportamentos an√¥malos ou regimes de mercado**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ================================================================\n",
    "# COEFICIENTE DE HURST / HURST EXPONENT\n",
    "# ================================================================\n",
    "def compute_hurst(series: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calcula o coeficiente de Hurst (mem√≥ria longa).\n",
    "    / Computes the Hurst exponent (long memory indicator).\n",
    "    \"\"\"\n",
    "    series = series[~np.isnan(series)]\n",
    "    # --> Remove valores ausentes / Remove NaN values <--\n",
    "\n",
    "    H, _, _ = compute_Hc(series, kind='price')\n",
    "    # --> Estima o coeficiente de Hurst usando o m√©todo 'price' / Estimates Hurst exponent with 'price' method <--\n",
    "\n",
    "    return H\n",
    "    # --> Retorna o valor estimado de H / Returns the estimated Hurst exponent <--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# CRUZAMENTOS COM A MEDIANA / MEDIAN CROSSINGS\n",
    "# ================================================================\n",
    "def count_median_crossings(series: np.ndarray) -> int:\n",
    "    \"\"\"\n",
    "    Conta quantos cruzamentos com a mediana ocorrem na s√©rie.\n",
    "    / Counts how many times the series crosses its median.\n",
    "    \"\"\"\n",
    "    median_val = np.median(series)\n",
    "    # --> Calcula a mediana da s√©rie / Computes series median <--\n",
    "\n",
    "    shifted = (series - median_val) > 0\n",
    "    # --> Converte a s√©rie para sinais positivos e negativos / Converts series to binary signal above/below median <--\n",
    "\n",
    "    return np.sum(shifted[1:] != shifted[:-1])\n",
    "    # --> Conta mudan√ßas de sinal (cruzamentos) / Counts sign changes (crossings) <--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# MAIOR REGI√ÉO DE ESTAGNA√á√ÉO / LONGEST FLAT SPOT\n",
    "# ================================================================\n",
    "def longest_flat_spot(series: np.ndarray, tol=1e-6) -> int:\n",
    "    \"\"\"\n",
    "    Encontra a maior sequ√™ncia de varia√ß√£o m√≠nima (estagna√ß√£o).\n",
    "    / Finds the longest sequence with minimal variation (stagnation).\n",
    "    \"\"\"\n",
    "    diff = np.abs(np.diff(series))\n",
    "    # --> Calcula a diferen√ßa absoluta entre observa√ß√µes consecutivas / Absolute difference between consecutive values <--\n",
    "\n",
    "    flat = diff < tol\n",
    "    # --> Marca onde a varia√ß√£o √© menor que a toler√¢ncia / Marks where variation is below tolerance <--\n",
    "\n",
    "    max_len = count = 0\n",
    "    # --> Inicializa contadores / Initializes counters <--\n",
    "\n",
    "    for val in flat:\n",
    "        count = count + 1 if val else 0\n",
    "        max_len = max(max_len, count)\n",
    "    # --> Atualiza o comprimento m√°ximo de sequ√™ncia plana / Updates max length of flat sequence <--\n",
    "\n",
    "    return max_len\n",
    "    # --> Retorna o comprimento da maior regi√£o de estagna√ß√£o / Returns length of longest stagnation segment <--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ==========================================================\n",
    "# C√°lculo do Coeficiente de Hurst com fallback para s√©ries curtas\n",
    "# Compute Hurst Exponent with fallback for short series\n",
    "# ==========================================================\n",
    "def compute_hurst(series, min_length=100):\n",
    "    \"\"\"\n",
    "    Calcula o coeficiente de Hurst se a s√©rie tiver pelo menos `min_length` pontos\n",
    "    Computes the Hurst exponent if the series has at least `min_length` points\n",
    "    \"\"\"\n",
    "    series = np.array(series)\n",
    "    series = series[~np.isnan(series)]\n",
    "    # --> Remove valores ausentes / Remove NaN values <--\n",
    "\n",
    "    if len(series) < min_length:\n",
    "        print(f\"[AVISO] S√©rie com apenas {len(series)} pontos. Retornando np.nan.\")\n",
    "        # --> Retorna NaN se a s√©rie for muito curta / Returns NaN if the series is too short <--\n",
    "        return np.nan\n",
    "\n",
    "    try:\n",
    "        H, _, _ = compute_Hc(series, kind='price')\n",
    "        # --> Estima o coeficiente de Hurst usando o m√©todo 'price' / Estimates Hurst exponent using 'price' method <--\n",
    "        return H\n",
    "    except Exception as e:\n",
    "        print(f\"[ERRO] Falha ao calcular Hurst: {e}\")\n",
    "        # --> Retorna NaN em caso de erro / Returns NaN if Hurst computation fails <--\n",
    "        return np.nan\n",
    "\n",
    "# ==========================================================\n",
    "# Contagem de cruzamentos com a mediana\n",
    "# Count crossings with the median\n",
    "# ==========================================================\n",
    "def count_median_crossings(series):\n",
    "    median = np.median(series)\n",
    "    # --> Calcula a mediana da s√©rie / Computes the median of the series <--\n",
    "\n",
    "    shifted = np.roll(series, 1)\n",
    "    # --> Desloca a s√©rie para comparar com o valor anterior / Shifts the series to compare with the previous value <--\n",
    "\n",
    "    crossings = (np.sign(series - median) != np.sign(shifted - median)).astype(int)\n",
    "    # --> Detecta mudan√ßa de sinal em rela√ß√£o √† mediana / Detects sign change with respect to the median <--\n",
    "\n",
    "    return np.sum(crossings[1:])\n",
    "    # --> Soma os cruzamentos, ignorando o primeiro valor deslocado / Sums the crossings, ignoring the first shifted value <--\n",
    "\n",
    "# ==========================================================\n",
    "# Maior sequ√™ncia com varia√ß√£o abaixo do limite (flat spot)\n",
    "# Longest low-variation sequence (flat spot)\n",
    "# ==========================================================\n",
    "def longest_flat_spot(series, tolerance=1e-5):\n",
    "    diffs = np.abs(np.diff(series))\n",
    "    # --> Calcula a diferen√ßa absoluta entre elementos consecutivos / Calculates absolute difference between consecutive elements <--\n",
    "\n",
    "    flat = diffs < tolerance\n",
    "    # --> Marca os pontos com varia√ß√£o muito baixa / Marks points with very low variation <--\n",
    "\n",
    "    max_len = count = 0\n",
    "    for val in flat:\n",
    "        count = count + 1 if val else 0\n",
    "        max_len = max(max_len, count)\n",
    "\n",
    "    return max_len + 1 if max_len > 0 else 0\n",
    "    # --> Retorna o comprimento m√°ximo da sequ√™ncia plana / Returns the max length of flat region <--\n",
    "\n",
    "# ==========================================================\n",
    "# APLICA√á√ÉO NA S√âRIE / APPLY TO YOUR SERIES\n",
    "# ==========================================================\n",
    "series_np = df_fft[\"btc_price_usd\"].dropna().values\n",
    "# --> Converte a s√©rie para array NumPy e remove NaNs / Converts series to NumPy array and drops NaNs <--\n",
    "\n",
    "hurst_val = compute_hurst(series_np)\n",
    "# --> Calcula o coeficiente de Hurst / Computes Hurst exponent <--\n",
    "\n",
    "crossings = count_median_crossings(series_np)\n",
    "# --> Conta os cruzamentos com a mediana / Counts median crossings <--\n",
    "\n",
    "flat_len = longest_flat_spot(series_np)\n",
    "# --> Calcula a maior sequ√™ncia com baixa varia√ß√£o / Computes longest flat spot <--\n",
    "\n",
    "print(\"Coeficiente de Hurst:\", hurst_val)\n",
    "# --> Exibe o coeficiente de Hurst / Prints Hurst exponent <--\n",
    "\n",
    "print(\"Cruzamentos com a mediana:\", crossings)\n",
    "# --> Exibe a contagem de cruzamentos / Prints median crossing count <--\n",
    "\n",
    "print(\"Maior regi√£o de estagna√ß√£o:\", flat_len)\n",
    "# --> Exibe o tamanho da maior sequ√™ncia estagnada / Prints longest flat segment length <--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# CONSTRU√á√ÉO DO DATAFRAME DE PERSIST√äNCIA / PERSISTENCE FEATURES\n",
    "# ================================================================\n",
    "\n",
    "df_persistence = pd.DataFrame([{\n",
    "    \"hurst_exponent\": hurst_val,\n",
    "    \"median_crossings\": crossings,\n",
    "    \"longest_flat_spot\": flat_len\n",
    "}])\n",
    "# --> Cria DataFrame com as features de persist√™ncia extra√≠das / Creates DataFrame with extracted persistence features <--\n",
    "\n",
    "# ================================\n",
    "# ADI√á√ÉO DAS CHAVES TEMPORAIS\n",
    "# ADDING TIMESTAMP AND BLOCK HEIGHT KEYS\n",
    "# ================================\n",
    "df_persistence[\"block_timestamp\"] = df_fft[\"block_timestamp\"].iloc[-1]\n",
    "# --> Adiciona o timestamp da √∫ltima janela usada / Adds timestamp of the last window used <--\n",
    "\n",
    "df_persistence[\"block_height\"] = df_fft[\"block_height\"].iloc[-1]\n",
    "# --> Adiciona a altura do bloco correspondente / Adds corresponding block height <--\n",
    "\n",
    "# ================================\n",
    "# CONSTRU√á√ÉO DO DATAFRAME FINAL\n",
    "# FINAL DISPLAY\n",
    "# ================================\n",
    "display(df_persistence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.6 Estabilidade Local e Heterocedasticidade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Vari√¢ncia de Blocos (var_tiled_var)\n",
    "\n",
    "Divide a s√©rie em blocos consecutivos e calcula a vari√¢ncia dentro de cada bloco. A **variabilidade dessas vari√¢ncias** indica **instabilidade ao longo do tempo**.\n",
    "\n",
    "$\n",
    "\\text{VarBloco}_i = \\text{Var}(X_{i1}, X_{i2}, ..., X_{im})\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "M√©dia de Blocos (var_tiled_mean)\n",
    "\n",
    "Calcula a m√©dia de cada bloco e depois mede a **vari√¢ncia entre essas m√©dias**. √â √∫til para identificar **mudan√ßas no n√≠vel m√©dio da s√©rie ao longo do tempo**.\n",
    "\n",
    "$\n",
    "\\text{MeanBloco}_i = \\frac{1}{m} \\sum_{j=1}^{m} X_{ij}\n",
    "\\quad \\Rightarrow \\quad \\text{Var}\\left( \\text{MeanBloco}_i \\right)\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "Teste ARCH (Engle LM)\n",
    "\n",
    "O **teste ARCH** verifica se os res√≠duos t√™m **heterocedasticidade condicional**, ou seja, **vari√¢ncia que muda ao longo do tempo**.  \n",
    "√â usado para avaliar se modelos como **GARCH** s√£o apropriados.\n",
    "\n",
    "$\n",
    "\\text{H}_0: \\text{N√£o h√° efeito ARCH (vari√¢ncia constante)}\n",
    "\\quad \\text{vs} \\quad\n",
    "\\text{H}_1: \\text{Existe heterocedasticidade condicional}\n",
    "$\n",
    "\n",
    "Resultado:\n",
    "\n",
    "- **Estat√≠stica LM**\n",
    "- **Valor-p** (se < 0.05, rejeita-se $ H_0 $)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# VARI√ÇNCIA DAS VARI√ÇNCIAS POR BLOCO / VARIANCE OF BLOCK-WISE VARIANCES\n",
    "# ================================================================\n",
    "def var_tiled_var(series: np.ndarray, block_size: int = 10) -> float:\n",
    "    \"\"\"\n",
    "    Vari√¢ncia das vari√¢ncias por blocos / Variance of block-wise variances.\n",
    "    \"\"\"\n",
    "    if isinstance(series, pd.Series):\n",
    "        series = series.dropna().values\n",
    "    else:\n",
    "        series = series[~np.isnan(series)]\n",
    "    # --> Garante array NumPy e remove NaNs / Ensures NumPy array and drops NaNs <--\n",
    "\n",
    "    trimmed = series[:len(series) // block_size * block_size]\n",
    "    # --> Ajusta o tamanho da s√©rie para ser divis√≠vel pelo tamanho do bloco / Trims series to be divisible by block size <--\n",
    "\n",
    "    blocks = trimmed.reshape(-1, block_size)\n",
    "    # --> Reshape da s√©rie em blocos n√£o sobrepostos / Reshapes series into non-overlapping blocks <--\n",
    "\n",
    "    block_vars = np.var(blocks, axis=1, ddof=1)\n",
    "    # --> Calcula a vari√¢ncia de cada bloco / Computes variance within each block <--\n",
    "\n",
    "    return np.var(block_vars, ddof=1)\n",
    "    # --> Retorna a vari√¢ncia das vari√¢ncias dos blocos / Returns variance of block-wise variances <--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# VARI√ÇNCIA DAS M√âDIAS POR BLOCO / VARIANCE OF BLOCK-WISE MEANS\n",
    "# ================================================================\n",
    "def var_tiled_mean(series: np.ndarray, block_size: int = 10) -> float:\n",
    "    \"\"\"\n",
    "    Vari√¢ncia das m√©dias por blocos / Variance of block-wise means.\n",
    "    \"\"\"\n",
    "    if isinstance(series, pd.Series):\n",
    "        series = series.dropna().values\n",
    "    else:\n",
    "        series = series[~np.isnan(series)]\n",
    "    # --> Garante array NumPy e remove NaNs / Ensures NumPy array and drops NaNs <--\n",
    "\n",
    "    trimmed = series[:len(series) // block_size * block_size]\n",
    "    # --> Ajusta o comprimento para ser divis√≠vel pelo tamanho do bloco / Trims to fit block size <--\n",
    "\n",
    "    blocks = trimmed.reshape(-1, block_size)\n",
    "    # --> Divide em blocos consecutivos n√£o sobrepostos / Reshapes into consecutive blocks <--\n",
    "\n",
    "    block_means = np.mean(blocks, axis=1)\n",
    "    # --> Calcula a m√©dia de cada bloco / Computes mean of each block <--\n",
    "\n",
    "    return np.var(block_means, ddof=1)\n",
    "    # --> Retorna a vari√¢ncia entre as m√©dias dos blocos / Returns variance of block-wise means <--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# TESTE DE HETEROCEDASTICIDADE ARCH / ARCH EFFECT TEST (ENGLE LM)\n",
    "# ================================================================\n",
    "def arch_test(series: np.ndarray, lags: int = 12) -> dict:\n",
    "    \"\"\"\n",
    "    Teste de heterocedasticidade ARCH (Engle LM).\n",
    "    / ARCH effect test using Engle's Lagrange Multiplier.\n",
    "    \"\"\"\n",
    "    series = series[~np.isnan(series)]\n",
    "    # --> Remove valores ausentes / Remove NaNs <--\n",
    "\n",
    "    lm_stat, lm_pvalue, _, _ = het_arch(series, nlags=lags)\n",
    "    # --> Executa o teste LM com n√∫mero definido de lags / Runs ARCH LM test with defined lags <--\n",
    "\n",
    "    return {\n",
    "        \"arch_lm_stat\": lm_stat,\n",
    "        # --> Estat√≠stica do teste LM / LM test statistic <--\n",
    "        \"arch_pvalue\": lm_pvalue\n",
    "        # --> Valor-p do teste / p-value from the test <--\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# FUN√á√ÉO AUXILIAR: VARI√ÇNCIA ENTRE M√âDIAS DE BLOCOS\n",
    "# AUXILIARY FUNCTION: VARIANCE ACROSS BLOCK MEANS\n",
    "# ================================================================\n",
    "def var_tiled_mean(series, block_size):\n",
    "    series = np.array(series)\n",
    "    series = series[~np.isnan(series)]\n",
    "    # --> Remove NaNs / Drop NaNs <--\n",
    "\n",
    "    if len(series) < block_size:\n",
    "        return np.nan\n",
    "        # --> S√©rie muito curta para blocos / Too short for block splitting <--\n",
    "\n",
    "    num_blocks = len(series) // block_size\n",
    "    blocks = series[:num_blocks * block_size].reshape(num_blocks, block_size)\n",
    "    block_means = blocks.mean(axis=1)\n",
    "    return np.var(block_means)\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# FUN√á√ÉO AUXILIAR: VARI√ÇNCIA ENTRE VARI√ÇNCIAS DE BLOCOS\n",
    "# AUXILIARY FUNCTION: VARIANCE ACROSS BLOCK VARIANCES\n",
    "# ================================================================\n",
    "def var_tiled_var(series, block_size):\n",
    "    series = np.array(series)\n",
    "    series = series[~np.isnan(series)]\n",
    "    # --> Remove NaNs / Drop NaNs <--\n",
    "\n",
    "    if len(series) < block_size:\n",
    "        return np.nan\n",
    "        # --> S√©rie muito curta para blocos / Too short for block splitting <--\n",
    "\n",
    "    num_blocks = len(series) // block_size\n",
    "    blocks = series[:num_blocks * block_size].reshape(num_blocks, block_size)\n",
    "    block_vars = blocks.var(axis=1)\n",
    "    return np.var(block_vars)\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# FUN√á√ÉO AUXILIAR: TESTE ARCH COM PROTE√á√ÉO\n",
    "# AUXILIARY FUNCTION: ARCH TEST WITH VALIDATION\n",
    "# ================================================================\n",
    "def arch_test(series, lags):\n",
    "    \"\"\"\n",
    "    Executa o teste ARCH se houver observa√ß√µes suficientes.\n",
    "    Runs ARCH test if series is long enough.\n",
    "    \"\"\"\n",
    "    if len(series) <= lags:\n",
    "        print(f\"[AVISO] S√©rie com {len(series)} pontos < n√∫mero de lags ({lags}). ARCH n√£o ser√° executado.\")\n",
    "        return {\"arch_lm_stat\": np.nan, \"arch_pvalue\": np.nan}\n",
    "        # --> Retorna NaNs se s√©rie for curta / Returns NaNs for short series <--\n",
    "\n",
    "    try:\n",
    "        arch_lm_stat, arch_pvalue, _, _ = het_arch(series, maxlag=lags)\n",
    "        return {\"arch_lm_stat\": arch_lm_stat, \"arch_pvalue\": arch_pvalue}\n",
    "        # --> Executa o teste e retorna estat√≠sticas / Runs ARCH test and returns stats <--\n",
    "    except Exception as e:\n",
    "        print(f\"[ERRO] Falha no teste ARCH: {e}\")\n",
    "        return {\"arch_lm_stat\": np.nan, \"arch_pvalue\": np.nan}\n",
    "        # --> Em caso de erro, retorna NaNs / On error, returns NaNs <--\n",
    "\n",
    "# ================================================================\n",
    "# EXTRA√á√ÉO DE FEATURES DE VOLATILIDADE / VOLATILITY FEATURE EXTRACTION\n",
    "# ================================================================\n",
    "def extract_volatility_features(\n",
    "    series: pd.Series,\n",
    "    block_height: int,\n",
    "    block_timestamp: pd.Timestamp,\n",
    "    block_size: int = 10,\n",
    "    arch_lags: int = 12\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extrai features de estabilidade local e heterocedasticidade.\n",
    "    / Extracts local variance and heteroskedasticity features.\n",
    "\n",
    "    Par√¢metros / Parameters:\n",
    "    - series: pd.Series ou np.ndarray\n",
    "        S√©rie temporal num√©rica / Numeric time series.\n",
    "    - block_height: int\n",
    "        Altura do bloco associada √† janela / Block height for tracking.\n",
    "    - block_timestamp: pd.Timestamp\n",
    "        Timestamp associado √† janela / Timestamp for tracking.\n",
    "    - block_size: int\n",
    "        Tamanho dos blocos para an√°lise local / Block size for local analysis.\n",
    "    - arch_lags: int\n",
    "        N√∫mero de lags para o teste ARCH / Number of lags for ARCH test.\n",
    "\n",
    "    Retorna / Returns:\n",
    "    - pd.DataFrame com uma linha contendo: block_height, block_timestamp, var_tiled_mean, var_tiled_var, arch_lm_stat, arch_pvalue\n",
    "    / One-row DataFrame with volatility and heteroskedasticity features.\n",
    "    \"\"\"\n",
    "\n",
    "    # ===========================\n",
    "    # PREPARA√á√ÉO DA S√âRIE / SERIES PREPROCESSING\n",
    "    # ===========================\n",
    "    if isinstance(series, pd.Series):\n",
    "        series = series.dropna().values\n",
    "        # --> Converte pd.Series para array e remove NaNs / Convert Series to array and drop NaNs <--\n",
    "    else:\n",
    "        series = series[~np.isnan(series)]\n",
    "        # --> Remove NaNs de array NumPy / Drop NaNs from NumPy array <--\n",
    "\n",
    "    # ===========================\n",
    "    # C√ÅLCULO DAS FEATURES / FEATURE EXTRACTION\n",
    "    # ===========================\n",
    "    features = {\n",
    "        \"var_tiled_mean\": var_tiled_mean(series, block_size),\n",
    "        # --> Vari√¢ncia entre m√©dias de blocos / Variance across block means <--\n",
    "\n",
    "        \"var_tiled_var\": var_tiled_var(series, block_size),\n",
    "        # --> Vari√¢ncia entre vari√¢ncias de blocos / Variance across block variances <--\n",
    "\n",
    "        **arch_test(series, lags=arch_lags)\n",
    "        # --> Estat√≠sticas do teste ARCH / ARCH test metrics <--\n",
    "    }\n",
    "\n",
    "    # ===========================\n",
    "    # FORMATA√á√ÉO FINAL / FINAL OUTPUT\n",
    "    # ===========================\n",
    "    features[\"block_height\"] = block_height\n",
    "    features[\"block_timestamp\"] = block_timestamp\n",
    "    # --> Adiciona metadados para rastreio / Adds tracking metadata <--\n",
    "\n",
    "    return pd.DataFrame([features])\n",
    "    # --> Retorna resultado como DataFrame de uma linha / Returns one-row DataFrame with features <--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = df_returns[\"log_return\"].iloc[-96:]\n",
    "block_height_ref = df_returns[\"block_height\"].iloc[-1]\n",
    "block_timestamp_ref = df_returns[\"block_timestamp\"].iloc[-1]\n",
    "\n",
    "df_vol_features = extract_volatility_features(\n",
    "    series=window,\n",
    "    block_height=block_height_ref,\n",
    "    block_timestamp=block_timestamp_ref,\n",
    "    block_size=10,\n",
    "    arch_lags=12\n",
    ")\n",
    "display(df_vol_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.7 Mudan√ßas Estruturais e Quebras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "Mudan√ßa de N√≠vel (shift_level_max)\n",
    "\n",
    "Compara **janelas consecutivas da s√©rie** e identifica a maior diferen√ßa entre suas m√©dias. √â usada para encontrar **saltos ou quedas abruptas** na tend√™ncia.\n",
    "\n",
    "$\n",
    "\\text{shift\\_level}_t = \\left| \\text{m√©dia}(X_{t:t+w}) - \\text{m√©dia}(X_{t+w:t+2w}) \\right|\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "Mudan√ßa de Vari√¢ncia (shift_var_max)\n",
    "\n",
    "Similar √† anterior, mas avalia a **diferen√ßa na vari√¢ncia** entre duas janelas consecutivas, revelando mudan√ßas de **volatilidade**.\n",
    "\n",
    "$\n",
    "\\text{shift\\_var}_t = \\left| \\text{var}(X_{t:t+w}) - \\text{var}(X_{t+w:t+2w}) \\right|\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "Mudan√ßa de Distribui√ß√£o (KL Divergence)\n",
    "\n",
    "Utiliza a **diverg√™ncia de Kullback-Leibler** para quantificar o qu√£o diferente √© a distribui√ß√£o de duas janelas consecutivas. Quanto maior, maior a **ruptura distribucional**.\n",
    "\n",
    "$\n",
    "\\text{KL}(P || Q) = \\sum_i P(i) \\log \\left( \\frac{P(i)}{Q(i)} \\right)\n",
    "$\n",
    "\n",
    "- $ P $: distribui√ß√£o na janela atual  \n",
    "- $ Q $: distribui√ß√£o na pr√≥xima janela  \n",
    "- Resultado: √≠ndice do maior KL + valor m√°ximo\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# DIVERG√äNCIA DE KULLBACK-LEIBLER / KULLBACK-LEIBLER DIVERGENCE\n",
    "# ================================================================\n",
    "def kl_divergence(p, q):\n",
    "    \"\"\"\n",
    "    Calcula a diverg√™ncia de KL entre duas distribui√ß√µes.\n",
    "    / Computes the KL divergence between two distributions.\n",
    "    \"\"\"\n",
    "    p = np.asarray(p) + 1e-12\n",
    "    q = np.asarray(q) + 1e-12\n",
    "    # --> Garante que n√£o h√° divis√£o por zero / Ensures no division by zero <--\n",
    "\n",
    "    return np.sum(p * np.log(p / q))\n",
    "    # --> Soma da diverg√™ncia ponto a ponto / Sum of pointwise divergence <--\n",
    "\n",
    "# ================================================================\n",
    "# MUDAN√áAS ESTRUTURAIS NA S√âRIE TEMPORAL / STRUCTURAL SHIFTS IN TIME SERIES\n",
    "# ================================================================\n",
    "def extract_structural_shifts(\n",
    "    series: pd.Series,\n",
    "    block_height: int,\n",
    "    block_timestamp: pd.Timestamp,\n",
    "    window: int = 30,\n",
    "    bins: int = 20\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Detecta rupturas estruturais usando m√©dia, vari√¢ncia e diverg√™ncia KL.\n",
    "    / Detects structural shifts using mean, variance, and KL divergence.\n",
    "    \"\"\"\n",
    "\n",
    "    series = series.dropna().values\n",
    "    # --> Remove valores ausentes e converte para array / Clean and convert to NumPy array <--\n",
    "\n",
    "    n = len(series)\n",
    "    if n < 2 * window:\n",
    "        print(f\"[AVISO] S√©rie com {n} pontos < 2x janela interna ({window}). Retornando linha vazia.\")\n",
    "        return pd.DataFrame([{\n",
    "            \"block_height\": block_height,\n",
    "            \"block_timestamp\": block_timestamp,\n",
    "            \"shift_level_max\": np.nan,\n",
    "            \"shift_var_max\": np.nan,\n",
    "            \"shift_kl_max\": np.nan\n",
    "        }])\n",
    "        # --> S√©rie muito curta: retorna linha com NaNs / Too short: return row with NaNs <--\n",
    "\n",
    "    shift_level = []\n",
    "    shift_var = []\n",
    "    shift_kl = []\n",
    "    # --> Inicializa listas para mudan√ßas / Initialize lists <--\n",
    "\n",
    "    # ================================\n",
    "    # LA√áOS DE JANELAS / LOOP THROUGH WINDOWS\n",
    "    # ================================\n",
    "    for i in range(n - 2 * window):\n",
    "        win1 = series[i : i + window]\n",
    "        win2 = series[i + window : i + 2 * window]\n",
    "        # --> Divide a s√©rie em duas janelas consecutivas / Two consecutive windows <--\n",
    "\n",
    "        shift_level.append(np.abs(np.mean(win1) - np.mean(win2)))\n",
    "        shift_var.append(np.abs(np.var(win1) - np.var(win2)))\n",
    "\n",
    "        hist1, _ = np.histogram(win1, bins=bins, density=True)\n",
    "        hist2, _ = np.histogram(win2, bins=bins, density=True)\n",
    "        shift_kl.append(kl_divergence(hist1, hist2))\n",
    "        # --> Diverg√™ncia KL entre distribui√ß√µes normalizadas / KL divergence between histograms <--\n",
    "\n",
    "    # ================================\n",
    "    # CRIA DATAFRAME COM RESULTADO AGREGADO / CREATE AGGREGATED RESULT\n",
    "    # ================================\n",
    "    df_shifts = pd.DataFrame([{\n",
    "        \"block_height\": block_height,\n",
    "        \"block_timestamp\": block_timestamp,\n",
    "        \"shift_level_max\": np.max(shift_level) if shift_level else np.nan,\n",
    "        \"shift_var_max\": np.max(shift_var) if shift_var else np.nan,\n",
    "        \"shift_kl_max\": np.max(shift_kl) if shift_kl else np.nan\n",
    "    }])\n",
    "    # --> Agrega m√°ximos das m√©tricas de mudan√ßa / Aggregate max values of shift metrics <--\n",
    "\n",
    "    return df_shifts\n",
    "    # --> Retorna DataFrame com uma linha de m√©tricas / Returns one-row DataFrame with metrics <--\n",
    "\n",
    "# ================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXECU√á√ÉO EM JANELAS DESLIZANTES / SLIDING WINDOW EXECUTION\n",
    "# ================================================================\n",
    "window_size = 60   # --> Tamanho da janela total / Total window size\n",
    "step = 10          # --> Passo entre janelas / Step between windows\n",
    "inner_window = 15  # --> Janela interna para compara√ß√£o / Internal window size\n",
    "shifts = []        # --> Lista para armazenar resultados / List to store results\n",
    "\n",
    "for i in range(0, len(df_fft) - window_size + 1, step):\n",
    "    sub_series = df_fft[\"btc_price_usd\"].iloc[i:i + window_size]\n",
    "    block_height = df_fft[\"block_height\"].iloc[i + window_size - 1]\n",
    "    block_timestamp = df_fft[\"block_timestamp\"].iloc[i + window_size - 1]\n",
    "    # --> Extrai informa√ß√µes finais da janela / Extract window metadata <--\n",
    "\n",
    "    shift_feats = extract_structural_shifts(\n",
    "        series=sub_series,\n",
    "        block_height=block_height,\n",
    "        block_timestamp=block_timestamp,\n",
    "        window=inner_window  # --> Janela interna de compara√ß√£o / Inner window size <--\n",
    "    )\n",
    "\n",
    "    shifts.append(shift_feats)\n",
    "\n",
    "# ================================================================\n",
    "# CONSOLIDA√á√ÉO FINAL / FINAL CONSOLIDATION\n",
    "# ================================================================\n",
    "if shifts:\n",
    "    df_shifts = pd.concat(shifts, ignore_index=True)\n",
    "    df_shifts = df_shifts.sort_values(\"block_timestamp\")\n",
    "    # --> Ordena resultado final / Sort final result <--\n",
    "    display(df_shifts.head())\n",
    "else:\n",
    "    print(\"[AVISO] Nenhuma janela v√°lida para extrair mudan√ßas estruturais.\")\n",
    "    df_shifts = pd.DataFrame()\n",
    "    # --> Se nenhuma janela for v√°lida, retorna DataFrame vazio / If no valid window, return empty DataFrame <--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. An√°lise de Estacionariedade e Transforma√ß√µes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Transforma√ß√µes da S√©rie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diferencia√ß√£o de Primeira e Segunda Ordem\n",
    "\n",
    "A diferencia√ß√£o √© uma t√©cnica para remover tend√™ncia e tornar a s√©rie mais estacion√°ria.\n",
    "\n",
    "- **Primeira diferen√ßa**:\n",
    "$\n",
    "Z'_t = Z_t - Z_{t-1}\n",
    "$\n",
    "\n",
    "- **Segunda diferen√ßa** (ap√≥s aplicar a primeira):\n",
    "$\n",
    "Z''_t = Z'_t - Z'_{t-1} = Z_t - 2Z_{t-1} + Z_{t-2}\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "Compara√ß√£o: S√©rie Original vs Diferenciada\n",
    "\n",
    "√â importante visualizar e comparar:\n",
    "\n",
    "- A s√©rie original  \n",
    "- A s√©rie com primeira e segunda diferen√ßa  \n",
    "- A estabiliza√ß√£o da m√©dia e da vari√¢ncia\n",
    "\n",
    "---\n",
    "\n",
    "Transforma√ß√£o Box-Cox com \\( \\lambda \\) √≥timo via Guerrero\n",
    "\n",
    "A transforma√ß√£o de Box-Cox torna a s√©rie **mais sim√©trica** e **estabiliza a vari√¢ncia**, essencial antes da diferencia√ß√£o.\n",
    "\n",
    "$\n",
    "Y_t^{(\\lambda)} = \n",
    "\\begin{cases}\n",
    "\\frac{Y_t^\\lambda - 1}{\\lambda}, & \\lambda \\ne 0 \\\\\n",
    "\\ln(Y_t), & \\lambda = 0\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "O **m√©todo de Guerrero** encontra o valor de \\( \\lambda \\) que minimiza a vari√¢ncia relativa das m√©dias m√≥veis de segmentos consecutivos.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from scipy.stats import boxcox, boxcox_normmax\n",
    "from scipy.special import boxcox1p\n",
    "from sktime.transformations.series.boxcox import BoxCoxTransformer\n",
    "\n",
    "# ================================================================\n",
    "# TRANSFORMA√á√ÉO BOX-COX COM MLE / BOX-COX TRANSFORMATION VIA MLE\n",
    "# ================================================================\n",
    "def boxcox_mle(series: pd.Series) -> tuple:\n",
    "    \"\"\"\n",
    "    Aplica Box-Cox com lambda √≥timo via m√°xima verossimilhan√ßa (MLE)\n",
    "    / Applies Box-Cox with optimal lambda via Maximum Likelihood Estimation\n",
    "    \"\"\"\n",
    "    series = series.dropna()\n",
    "    # --> Remove valores ausentes antes de aplicar a transforma√ß√£o / Drop missing values before transformation <--\n",
    "\n",
    "    lambda_opt = boxcox_normmax(series, method=\"mle\")\n",
    "    # --> Estima o lambda √≥timo via MLE / Estimate optimal lambda using MLE <--\n",
    "\n",
    "    transformed = boxcox(series, lmbda=lambda_opt)\n",
    "    # --> Aplica a transforma√ß√£o de Box-Cox / Apply Box-Cox transformation <--\n",
    "\n",
    "    return transformed, lambda_opt\n",
    "    # --> Retorna a s√©rie transformada e o lambda estimado / Return transformed series and lambda <--\n",
    "\n",
    "# ================================================================\n",
    "# TRANSFORMA√á√ïES COMPLETAS DA S√âRIE / COMPLETE SERIES TRANSFORMATION\n",
    "# ================================================================\n",
    "def apply_series_transformations(series: pd.Series, seasonal_period: int = 12) -> dict:\n",
    "    \"\"\"\n",
    "    Aplica transforma√ß√µes: Box-Cox, diferen√ßas de 1¬™ e 2¬™ ordem\n",
    "    / Applies Box-Cox, 1st and 2nd order differencing\n",
    "    \"\"\"\n",
    "    series = series[series > 0].dropna()\n",
    "    # --> Remove zeros ou valores negativos e NaNs (Box-Cox exige valores positivos) / Remove zeros, negatives, and NaNs <--\n",
    "\n",
    "    boxcox_series, lambda_opt = boxcox_mle(series)\n",
    "    # --> Aplica a transforma√ß√£o Box-Cox via MLE / Apply Box-Cox transformation using MLE <--\n",
    "\n",
    "    diff1 = pd.Series(np.diff(boxcox_series), index=series.index[1:])\n",
    "    # --> Calcula a 1¬™ diferen√ßa da s√©rie transformada / Compute 1st order differencing <--\n",
    "\n",
    "    diff2 = pd.Series(np.diff(diff1), index=series.index[2:])\n",
    "    # --> Calcula a 2¬™ diferen√ßa da s√©rie transformada / Compute 2nd order differencing <--\n",
    "\n",
    "    return {\n",
    "        \"original\": series,                                         # S√©rie original / Original series\n",
    "        \"boxcox_transformed\": pd.Series(boxcox_series, index=series.index),  # S√©rie transformada / Transformed series\n",
    "        \"lambda_boxcox\": lambda_opt,                                # Lambda √≥timo estimado / Estimated optimal lambda\n",
    "        \"diff_1st\": diff1,                                          # Primeira diferen√ßa / First difference\n",
    "        \"diff_2nd\": diff2                                           # Segunda diferen√ßa / Second difference\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = apply_series_transformations(df_fft[\"btc_price_usd\"])\n",
    "print(\"Lambda √≥timo:\", result[\"lambda_boxcox\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = apply_series_transformations(df_fft[\"btc_price_usd\"])\n",
    "plot_transformed_series(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Diagn√≥stico de Estacionariedade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "An√°lise Gr√°fica de ACF e PACF\n",
    "\n",
    "A **Autocorrela√ß√£o (ACF)** mostra o quanto a s√©rie atual depende de seus pr√≥prios lags.  \n",
    "A **Autocorrela√ß√£o Parcial (PACF)** mostra a correla√ß√£o dos lags com a s√©rie, **removendo a influ√™ncia dos lags anteriores**.\n",
    "\n",
    "---\n",
    "\n",
    "Teste Dickey-Fuller Aumentado (ADF)\n",
    "\n",
    "Hip√≥teses:\n",
    "- $ H_0 $: A s√©rie **possui raiz unit√°ria** (n√£o estacion√°ria)\n",
    "- $ H_1 $: A s√©rie **√© estacion√°ria**\n",
    "\n",
    "Estat√≠stica de teste:  \n",
    "$\n",
    "\\Delta Z_t = \\alpha + \\beta t + \\gamma Z_{t-1} + \\sum \\delta_i \\Delta Z_{t-i} + \\varepsilon_t\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "Teste KPSS\n",
    "\n",
    "Hip√≥teses:\n",
    "- $ H_0 $: A s√©rie **√© estacion√°ria**\n",
    "- $ H_1 $: A s√©rie **n√£o √© estacion√°ria**\n",
    "\n",
    "Complementa o ADF para evitar conclus√µes enviesadas.\n",
    "\n",
    "---\n",
    "\n",
    "Teste Phillips-Perron (PP)\n",
    "\n",
    "Similar ao ADF, mas **mais robusto √† heterocedasticidade e autocorrela√ß√£o nos res√≠duos**.\n",
    "\n",
    "---\n",
    "\n",
    "Estimativas √ìtimas de Diferen√ßa (unitroot_ndiffs)\n",
    "\n",
    "- **unitroot_ndiffs** ‚Üí n√∫mero ideal de diferencia√ß√µes para estacionarizar (n√£o sazonal)\n",
    "- **unitroot_nsdiffs** ‚Üí n√∫mero ideal de diferencia√ß√µes sazonais\n",
    "\n",
    "---\n",
    "\n",
    "Rolling M√©dia e Desvio\n",
    "\n",
    "Verifica se a m√©dia e vari√¢ncia **permanecem est√°veis ao longo do tempo**, outro indicativo visual de estacionariedade.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "\n",
    "# ================================================================\n",
    "# ESTIMATIVA DE NDIFs PELO TESTE ADF / NDIFs ESTIMATION VIA ADF TEST\n",
    "# ================================================================\n",
    "def estimate_ndiffs_adf(series, max_d=3, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Estima o n√∫mero m√≠nimo de diferencia√ß√µes necess√°rias para estacionarizar via ADF.\n",
    "    / Estimates the minimum number of differences needed for stationarity using the ADF test.\n",
    "\n",
    "    Par√¢metros / Parameters:\n",
    "    - series: pd.Series\n",
    "        S√©rie temporal a ser testada / Time series to be tested.\n",
    "    - max_d: int\n",
    "        N√∫mero m√°ximo de diferencia√ß√µes permitidas / Maximum number of differences allowed.\n",
    "    - alpha: float\n",
    "        N√≠vel de signific√¢ncia para o teste ADF / Significance level for ADF test.\n",
    "\n",
    "    Retorna / Returns:\n",
    "    - int: n√∫mero de diferencia√ß√µes necess√°rias / Number of differences required.\n",
    "    \"\"\"\n",
    "    for d in range(max_d + 1):\n",
    "        test_series = series if d == 0 else series.diff(d).dropna()\n",
    "        # --> Aplica diferencia√ß√£o d vezes / Apply differencing d times <--\n",
    "\n",
    "        pval = adfuller(test_series)[1]\n",
    "        # --> Extrai o valor-p do teste ADF / Extract p-value from ADF test <--\n",
    "\n",
    "        if pval < alpha:\n",
    "            return d\n",
    "        # --> Retorna o n√∫mero de diferen√ßas se a s√©rie for estacion√°ria / Return d if stationary <--\n",
    "\n",
    "    return max_d\n",
    "    # --> Retorna o valor m√°ximo se nenhuma diferencia√ß√£o atender ao crit√©rio / Return max_d if no d meets criteria <--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "\n",
    "# ================================================================\n",
    "# DIAGN√ìSTICO DE ESTACIONARIEDADE / STATIONARITY DIAGNOSTICS\n",
    "# ================================================================\n",
    "def stationarity_diagnostics(\n",
    "    series: pd.Series,\n",
    "    block_height: int,\n",
    "    block_timestamp: pd.Timestamp\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Executa diagn√≥stico b√°sico de estacionariedade (ADF, KPSS e ndiffs).\n",
    "    / Performs basic stationarity diagnostics (ADF, KPSS and estimated ndiffs).\n",
    "    \n",
    "    Par√¢metros / Parameters:\n",
    "    - series: pd.Series\n",
    "        S√©rie temporal a ser analisada / Time series to be analyzed.\n",
    "    - block_height: int\n",
    "        Altura do bloco associada √† janela / Block height for traceability.\n",
    "    - block_timestamp: pd.Timestamp\n",
    "        Timestamp da √∫ltima observa√ß√£o da janela / Timestamp of the last observation in the window.\n",
    "    \n",
    "    Retorna / Returns:\n",
    "    - pd.DataFrame com uma linha e m√©tricas de estacionariedade /\n",
    "      One-row DataFrame with stationarity diagnostics.\n",
    "    \"\"\"\n",
    "\n",
    "    # -----------------------\n",
    "    # PR√â-PROCESSAMENTO / PREPROCESSING\n",
    "    # -----------------------\n",
    "    series = series.dropna()\n",
    "    # --> Remove valores ausentes da s√©rie / Drop missing values <--\n",
    "\n",
    "    # -----------------------\n",
    "    # TESTE ADF / ADF TEST\n",
    "    # -----------------------\n",
    "    adf_result = adfuller(series)\n",
    "    adf_stat = adf_result[0]\n",
    "    adf_pvalue = adf_result[1]\n",
    "\n",
    "    # -----------------------\n",
    "    # TESTE KPSS / KPSS TEST\n",
    "    # -----------------------\n",
    "    kpss_result = kpss(series, regression=\"c\", nlags=\"legacy\")\n",
    "    kpss_stat = kpss_result[0]\n",
    "    kpss_pvalue = kpss_result[1]\n",
    "\n",
    "    # -----------------------\n",
    "    # ESTIMATIVA DE NDIFs / ESTIMATED DIFFERENCES\n",
    "    # -----------------------\n",
    "    d_est = estimate_ndiffs_adf(series)\n",
    "\n",
    "    # -----------------------\n",
    "    # DATAFRAME DE RESULTADO / RESULT DATAFRAME\n",
    "    # -----------------------\n",
    "    return pd.DataFrame([{\n",
    "        \"block_height\": block_height,\n",
    "        \"block_timestamp\": block_timestamp,\n",
    "        \"ADF_stat\": adf_stat,\n",
    "        \"ADF_pvalue\": adf_pvalue,\n",
    "        \"KPSS_stat\": kpss_stat,\n",
    "        \"KPSS_pvalue\": kpss_pvalue,\n",
    "        \"ndiffs_est_adf\": d_est\n",
    "    }])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import boxcox\n",
    "\n",
    "# Aplica Box-Cox\n",
    "df_transf = df_fft.copy()\n",
    "df_transf[\"boxcox_transformed\"], _ = boxcox(df_transf[\"btc_price_usd\"].dropna() + 1)\n",
    "\n",
    "# Plota diagn√≥sticos com a coluna criada\n",
    "plot_rolling_diagnostics_overlay(\n",
    "    series=df_transf[\"boxcox_transformed\"],\n",
    "    window=30,\n",
    "    title=\"Rolling M√©dia e Desvio - Box-Cox Transformada\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üî∂ ‚Çø -----> Diferencia√ß√£o da S√©rie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# CONFIGURA√á√ïES DE PAR√ÇMETROS\n",
    "# ============================\n",
    "resample_freq = 'h'            # Frequ√™ncia: 'h' (hora), 'd' (dia), 'W' (semana) / Resampling frequency\n",
    "nlags_option = 'auto'          # 'manual', 'serie_longa', 'auto' / Method to determine number of lags for ACF\n",
    "manual_nlags = 30              # Lag manual, se escolhido / Manual lag count if selected\n",
    "date_range_start = None        # Ex: \"2023-01-01\" / Optional start date filter\n",
    "date_range_end = None          # Ex: \"2023-03-01\" / Optional end date filter\n",
    "\n",
    "# ===========================\n",
    "# PR√â-PROCESSAMENTO DA S√âRIE\n",
    "# ===========================\n",
    "\n",
    "df_diff = df_features_temp.toPandas()\n",
    "# --> Converte o DataFrame PySpark para Pandas / Converts PySpark DataFrame to Pandas <--\n",
    "\n",
    "df_diff[\"block_timestamp\"] = pd.to_datetime(df_diff[\"block_timestamp\"])\n",
    "# --> Garante que o campo de tempo seja do tipo datetime / Ensures timestamp field is datetime type <--\n",
    "\n",
    "df_diff = df_diff.sort_values(\"block_timestamp\")\n",
    "# --> Ordena o DataFrame pelo tempo / Sorts the DataFrame by timestamp <--\n",
    "\n",
    "df_diff.set_index('block_timestamp', inplace=True)\n",
    "# --> Define o √≠ndice como timestamp (necess√°rio para o resample) / Sets timestamp as index (needed for resample) <--\n",
    "\n",
    "df_diff = df_diff.resample('d').mean()\n",
    "# --> Reamostra os dados com frequ√™ncia hor√°ria, calculando a m√©dia por hora / Resamples data to hourly frequency, taking the mean per hour <--\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# FILTRO DE INTERVALO DE DATAS (OPCIONAL)\n",
    "# ========================================\n",
    "if date_range_start and date_range_end:\n",
    "    df_diff = df_diff.loc[date_range_start:date_range_end]\n",
    "# --> Filtra o DataFrame por intervalo de datas, se fornecido / Filters data by date range if defined <--\n",
    "\n",
    "# =========================\n",
    "# DIFERENCIA√á√ÉO (1¬™ ordem)\n",
    "# =========================\n",
    "df_diff[\"btc_price_diff\"] = df_diff[\"btc_price_usd\"].diff()\n",
    "# --> Aplica diferencia√ß√£o de 1¬™ ordem: Y(t) = Z(t) - Z(t-1) / First-order differencing <--\n",
    "\n",
    "df_diff_stationary = df_diff.dropna()\n",
    "# --> Remove os valores nulos gerados pela diferencia√ß√£o / Drops NaNs from differencing <--\n",
    "\n",
    "# ============================\n",
    "# BUFFER VISUAL PARA GR√ÅFICOS\n",
    "# ============================\n",
    "y_min = df_diff[\"btc_price_usd\"].min() * 0.98\n",
    "y_max = df_diff[\"btc_price_usd\"].max() * 1.02\n",
    "# --> Define um intervalo visual com 2% de margem no eixo Y / Adds 2% buffer for Y-axis range <--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üî∂ ‚Çø -----> Modelagem Matem√°tica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Condi√ß√µes de Estacionariedade de segunda ordem (Fraca)\n",
    "\n",
    "Uma s√©rie temporal $ \\ Z(t) \\ $ √© dita **fracamente estacion√°ria** se satisfaz:\n",
    "\n",
    "1. M√©dia constante:  \n",
    "   $\n",
    "   \\mathbb{E}[Z(t)] = \\mu\n",
    "   $\n",
    "\n",
    "2. Vari√¢ncia constante:  \n",
    "   $\n",
    "   \\text{Var}(Z(t)) = \\sigma^2\n",
    "   $\n",
    "\n",
    "3. Autocovari√¢ncia depende apenas do lag \\( k \\):  \n",
    "   $\n",
    "   \\gamma(k) = \\text{Cov}(Z(t), Z(t+k)) = \\mathbb{E}[(Z(t) - \\mu)(Z(t+k) - \\mu)]\n",
    "   $\n",
    "---   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üî∂ ‚Çø -----> Plotagem da S√©rie Original e Diferenciada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_series_comparativa(\n",
    "    df_graph=df_diff.reset_index(),  # <-- devolve 'block_timestamp' como coluna\n",
    "    df_graph_stationary=df_diff_stationary.reset_index(),  # <-- idem\n",
    "    y_min=y_min,\n",
    "    y_max=y_max\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Diagn√≥stico de Estacionariedade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üî∂ ‚Çø -----> Modelagem Matem√°tica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Fun√ß√£o de Autocorrela√ß√£o (ACF)\n",
    "\n",
    "\n",
    "Dada uma s√©rie temporal $Z(t)$ com m√©dia $\\mu$ e vari√¢ncia $\\sigma^2$, a ACF no lag $k$ √© definida como:\n",
    "\n",
    "$\n",
    "\\rho(k) = \\frac{\\gamma(k)}{\\gamma(0)} = \\frac{\\mathbb{E}[(Z(t) - \\mu)(Z(t+k) - \\mu)]}{\\sigma^2}\n",
    "$\n",
    "\n",
    "Onde:  \n",
    "$\\rho(k)$ √© o valor da ACF no lag $k$   \n",
    "$\\gamma(k)$ √© a fun√ß√£o de autocovari√¢ncia no lag $k$  \n",
    "$\\gamma(0) = \\text{Var}(Z(t)) = \\sigma^2$  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üî∂ ‚Çø -----> An√°lise de Autocorrela√ß√£o (ACF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acf = df_features_temp.toPandas()\n",
    "# --> Converte o DataFrame do PySpark para Pandas / Converts PySpark DataFrame to Pandas <--\n",
    "\n",
    "df_acf[\"block_timestamp\"] = pd.to_datetime(df_acf[\"block_timestamp\"])\n",
    "# --> Garante que o campo de timestamp seja do tipo datetime / Ensures that 'block_timestamp' is in datetime format <--\n",
    "\n",
    "df_acf = df_acf.sort_values(\"block_timestamp\")\n",
    "# --> Ordena os dados cronologicamente / Sorts the data by timestamp <--\n",
    "\n",
    "df_acf.set_index('block_timestamp', inplace=True)\n",
    "# --> Define 'block_timestamp' como √≠ndice do DataFrame (necess√°rio para o resample) / Sets 'block_timestamp' as DataFrame index (required for resampling) <--\n",
    "\n",
    "df_acf = df_acf.resample('h').mean()\n",
    "# --> Reamostra os dados com frequ√™ncia hor√°ria, calculando a m√©dia por hora / Resamples data to hourly frequency, taking the mean per hour <--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üî∂ ‚Çø -----> Diferencia√ß√£o da S√©rie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# CONFIGURA√á√ïES INICIAIS\n",
    "# =======================\n",
    "\n",
    "resample_freq = 'd'           # 'h' = hora, 'd' = dia, 'W' = semana / frequency of resampling\n",
    "nlags_option = 'serie_longa'  # Op√ß√µes: 'manual', 'auto', 'serie_longa' / lag selection mode\n",
    "manual_nlags = 150            # Lag manual, se escolhido / manual lag value\n",
    "date_range_start = None       # Ex: \"2023-01-15\" / optional start date\n",
    "date_range_end = None         # Ex: \"2023-02-15\" / optional end date\n",
    "\n",
    "# ===============\n",
    "# PROCESSAMENTO\n",
    "# ===============\n",
    "\n",
    "df_acf = df_features_temp.toPandas()\n",
    "# --> Converte o DataFrame PySpark para Pandas / Converts PySpark DataFrame to Pandas <--\n",
    "\n",
    "df_acf[\"block_timestamp\"] = pd.to_datetime(df_acf[\"block_timestamp\"])\n",
    "# --> Garante que o campo de tempo seja datetime / Ensures the timestamp is datetime <--\n",
    "\n",
    "df_acf = df_acf.sort_values(\"block_timestamp\").set_index(\"block_timestamp\")\n",
    "# --> Ordena os dados cronologicamente e define o √≠ndice / Sorts data chronologically and sets index <--\n",
    "\n",
    "df_acf = df_acf.resample(resample_freq).mean()\n",
    "# --> Reamostra a s√©rie conforme a frequ√™ncia (di√°ria neste caso) / Resamples the series (daily in this case) <--\n",
    "\n",
    "if date_range_start and date_range_end:\n",
    "    df_acf = df_acf.loc[date_range_start:date_range_end]\n",
    "# --> Aplica filtro por intervalo de datas, se definido / Filters by date range, if defined <--\n",
    "\n",
    "# S√©rie diferenciada\n",
    "serie_diff = df_acf[\"btc_price_usd\"].diff().dropna()  # Y(t) = Z(t) - Z(t - 1)\n",
    "# --> Aplica diferencia√ß√£o de primeira ordem para obter s√©rie estacion√°ria / Applies first-order differencing <--\n",
    "\n",
    "# Sele√ß√£o do n√∫mero de defasagens\n",
    "if nlags_option == 'manual':\n",
    "    nlags = manual_nlags\n",
    "    # --> Define lag manualmente / Uses manual lag value <--\n",
    "elif nlags_option == 'serie_longa':\n",
    "    nlags = len(serie_diff) // 4\n",
    "    # --> Lag proporcional para s√©ries longas / Uses one-fourth of the series length <--\n",
    "elif nlags_option == 'auto':\n",
    "    nlags = len(serie_diff)\n",
    "    # --> Usa o comprimento total da s√©rie como nlags / Uses full length of the series <--\n",
    "else:\n",
    "    nlags = len(serie_diff)\n",
    "    # --> Valor padr√£o / Default fallback value <--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üî∂ ‚Çø -----> An√°lise Gr√°fica da ACF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf_diferenciada(serie_diff, nlags, nlags_option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "\n",
    "# ================================================================\n",
    "# EXTRA√á√ÉO DE ACF E PACF COM RASTREABILIDADE TEMPORAL\n",
    "# ACF & PACF EXTRACTION WITH TRACEABILITY\n",
    "# ================================================================\n",
    "def extract_acf_pacf_features(\n",
    "    series: pd.Series,\n",
    "    block_height: pd.Series,\n",
    "    block_timestamp: pd.Series,\n",
    "    window: int = 24\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extrai autocorrela√ß√µes (ACF e PACF) em janelas m√≥veis, com rastreabilidade temporal.\n",
    "    / Extracts autocorrelation and partial autocorrelation in rolling windows with temporal traceability.\n",
    "\n",
    "    Par√¢metros / Parameters:\n",
    "    - series: pd.Series\n",
    "        S√©rie temporal (ex: log-return) / Time series (e.g., log return).\n",
    "    - block_height: pd.Series\n",
    "        Altura do bloco para rastreamento / Block height for traceability.\n",
    "    - block_timestamp: pd.Series\n",
    "        Timestamp para rastreamento / Block timestamp for traceability.\n",
    "    - window: int\n",
    "        Tamanho da janela rolling / Rolling window size.\n",
    "\n",
    "    Retorna / Returns:\n",
    "    - pd.DataFrame com colunas: block_height, block_timestamp, acf_1, acf_5, acf_10, pacf_1\n",
    "    \"\"\"\n",
    "\n",
    "    # -----------------------\n",
    "    # Fun√ß√µes auxiliares para calcular ACF e PACF em cada janela\n",
    "    # / Helper functions to compute ACF and PACF for a given window\n",
    "    # -----------------------\n",
    "    def calc_acf(x, lag):\n",
    "        try:\n",
    "            return acf(x, nlags=lag, fft=False)[lag]\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "\n",
    "    def calc_pacf(x, lag):\n",
    "        try:\n",
    "            return pacf(x, nlags=lag)[lag]\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "\n",
    "    # -----------------------\n",
    "    # Inicializa DataFrame de resultados\n",
    "    # / Initialize output DataFrame\n",
    "    # -----------------------\n",
    "    results = []\n",
    "\n",
    "    for i in range(window, len(series)):\n",
    "        window_series = series.iloc[i - window:i]\n",
    "        bh = block_height.iloc[i]\n",
    "        ts = block_timestamp.iloc[i]\n",
    "\n",
    "        result = {\n",
    "            \"block_height\": bh,\n",
    "            \"block_timestamp\": ts,\n",
    "            \"acf_1\": calc_acf(window_series, 1),\n",
    "            \"acf_5\": calc_acf(window_series, 5),\n",
    "            \"acf_10\": calc_acf(window_series, 10),\n",
    "            \"pacf_1\": calc_pacf(window_series, 1)\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "    # -----------------------\n",
    "    # Converte lista em DataFrame final\n",
    "    # / Convert result list into final DataFrame\n",
    "    # -----------------------\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acf_pacf = extract_acf_pacf_features(\n",
    "    series=df_returns[\"log_return\"],\n",
    "    block_height=df_returns[\"block_height\"],\n",
    "    block_timestamp=df_returns[\"block_timestamp\"],\n",
    "    window=48\n",
    ")\n",
    "\n",
    "display(df_acf_pacf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üî∂ ‚Çø -----> Lembrete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Teste de Dickey-Fuller Aumentado (ADF)\n",
    "\n",
    "O Teste ADF verifica se uma s√©rie temporal possui raiz unit√°ria, ou seja, se n√£o √© estacion√°ria.\n",
    "\n",
    "A equa√ß√£o testada √©:\n",
    "\n",
    "$\n",
    "\\Delta Z_t = \\alpha + \\beta t + \\gamma Z_{t-1} + \\sum_{i=1}^{p} \\delta_i \\Delta Z_{t-i} + \\varepsilon_t\n",
    "$\n",
    "\n",
    "Onde:\n",
    "$Z_t$ ‚Üí valor da s√©rie no tempo $t$  \n",
    "$\\Delta Z_t = Z_t - Z_{t-1}$ ‚Üí primeira diferen√ßa  \n",
    "$\\alpha$ ‚Üí constante (termo de tend√™ncia)  \n",
    "$\\beta t$ ‚Üí tend√™ncia determin√≠stica (opcional)  \n",
    "  \n",
    "$\\gamma$ ‚Üí par√¢metro-chave:  \n",
    "Se $\\gamma = 0$, a s√©rie tem raiz unit√°ria (n√£o estacion√°ria)  \n",
    "Se $\\gamma < 0$, a s√©rie √© estacion√°ria  \n",
    "$\\sum \\delta_i \\Delta Z_{t-i}$ ‚Üí componentes autorregressivos adicionais (lags)  \n",
    "$\\varepsilon_t$ ‚Üí erro branco (white noise)  \n",
    "\n",
    "‚∏ª\n",
    "\n",
    "Interpreta√ß√£o\n",
    "\t‚Ä¢\tHip√≥tese nula ($H_0$): a s√©rie possui raiz unit√°ria ‚áí n√£o √© estacion√°ria\n",
    "\t‚Ä¢\tHip√≥tese alternativa ($H_1$): a s√©rie √© estacion√°ria\n",
    "\n",
    "Se p-valor < 0.05, rejeita-se $H_0$ ‚áí a s√©rie √© estacion√°ria.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üî∂ ‚Çø -----> Teste de Dickey-Fuller Aumentado (ADF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# TESTE DE ESTACIONARIEDADE (ADF) COM SUPORTE A S√âRIES CURTAS\n",
    "# ADF STATIONARITY TEST WITH SHORT SERIES SUPPORT\n",
    "# ================================================================\n",
    "def run_adf_test(series: pd.Series) -> dict:\n",
    "    \"\"\"\n",
    "    Executa o teste de Dickey-Fuller Aumentado com prote√ß√£o para s√©ries curtas.\n",
    "    / Runs Augmented Dickey-Fuller test with safe handling for short series.\n",
    "    \n",
    "    Retorna / Returns:\n",
    "    - dicion√°rio com estat√≠stica, p-valor e n√∫mero de lags usados.\n",
    "    / dictionary with test statistic, p-value, and number of lags used.\n",
    "    \"\"\"\n",
    "    series = series.dropna()\n",
    "    # --> Remove NaNs gerados pela diferencia√ß√£o / Drop NaNs from differenced series <--\n",
    "\n",
    "    if len(series) < 20:\n",
    "        print(f\"[AVISO] S√©rie muito curta para ADF (n = {len(series)}). Retornando NaNs.\")\n",
    "        return {\"adf_stat\": np.nan, \"p_value\": np.nan, \"used_lags\": np.nan}\n",
    "        # --> Evita execu√ß√£o do teste ADF com s√©ries curtas / Prevents running ADF on short series <--\n",
    "\n",
    "    try:\n",
    "        resultado = adfuller(series)\n",
    "        return {\n",
    "            \"adf_stat\": resultado[0],\n",
    "            \"p_value\": resultado[1],\n",
    "            \"used_lags\": resultado[2]\n",
    "        }\n",
    "        # --> Retorna os resultados principais do teste ADF / Returns ADF main outputs <--\n",
    "    except Exception as e:\n",
    "        print(f\"[ERRO] Falha no teste ADF: {e}\")\n",
    "        return {\"adf_stat\": np.nan, \"p_value\": np.nan, \"used_lags\": np.nan}\n",
    "        # --> Em caso de erro inesperado / In case of error, return NaNs <--\n",
    "\n",
    "# ================================================================\n",
    "# PR√â-PROCESSAMENTO DA S√âRIE PARA DIFERENCIA√á√ÉO\n",
    "# PREPROCESSING SERIES FOR DIFFERENCING\n",
    "# ================================================================\n",
    "df_acf[\"btc_price_diff\"] = df_acf[\"btc_price_usd\"].diff()\n",
    "# --> Aplica diferencia√ß√£o de primeira ordem para tornar a s√©rie estacion√°ria / Applies first-order differencing to stabilize the series <--\n",
    "\n",
    "df_acf_stationary = df_acf.dropna()\n",
    "# --> Remove os valores NaN resultantes da diferencia√ß√£o / Drops NaN values from differencing <--\n",
    "\n",
    "# ================================================================\n",
    "# EXECU√á√ÉO DO TESTE ADF / RUNNING THE ADF TEST\n",
    "# ================================================================\n",
    "resultado_adf = run_adf_test(df_acf_stationary[\"btc_price_diff\"])\n",
    "# --> Executa o teste de estacionariedade com valida√ß√£o / Runs ADF test with safe fallback <--\n",
    "\n",
    "# ================================================================\n",
    "# RESULTADOS / RESULTS\n",
    "# ================================================================\n",
    "print(\"ADF:\", resultado_adf[\"adf_stat\"])\n",
    "# --> Estat√≠stica do teste ADF (quanto mais negativa, mais estacion√°ria a s√©rie) / ADF test statistic <--\n",
    "\n",
    "print(\"p-valor:\", resultado_adf[\"p_value\"])\n",
    "# --> p-valor do teste ADF (menor que 0.05 sugere estacionariedade) / ADF p-value <--\n",
    "\n",
    "print(\"Usou at√©\", resultado_adf[\"used_lags\"], \"lags\")\n",
    "# --> N√∫mero de defasagens consideradas no modelo ADF / Number of lags used in ADF model <--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üî∂ ‚Çø -----> Lembrete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Diagn√≥stico Visual de Estacionariedade ‚Äî M√©dia e Desvio Padr√£o M√≥veis\n",
    "\n",
    "A s√©rie $Z(t)$ √© transformada por diferencia√ß√£o para an√°lise de estacionariedade. Em seguida, calcula-se:\n",
    "\t1.\tM√©dia M√≥vel (Rolling Mean):  \n",
    "\n",
    "$\n",
    "\\mu_t^{(w)} = \\frac{1}{w} \\sum_{i=0}^{w-1} Z(t - i)  \n",
    "$\n",
    "\n",
    "2.\tDesvio Padr√£o M√≥vel (Rolling Std): \n",
    "\n",
    "$\n",
    "\\sigma_t^{(w)} = \\sqrt{ \\frac{1}{w} \\sum_{i=0}^{w-1} \\left(Z(t - i) - \\mu_t^{(w)}\\right)^2 }  \n",
    "$\n",
    "\n",
    "Onde:\n",
    "$w$ = janela de tempo\n",
    "$\\mu_t^{(w)}$ = m√©dia m√≥vel no tempo $t$  \n",
    "$\\sigma_t^{(w)}$ = desvio padr√£o m√≥vel no tempo $t$  \n",
    "\n",
    "‚∏ª\n",
    "\n",
    "Interpreta√ß√£o\n",
    "\n",
    "Se ao longo do tempo:  \n",
    "A m√©dia $\\mu_t^{(w)}$ se mant√©m aproximadamente constante, e  \n",
    "O desvio padr√£o $\\sigma_t^{(w)}$ tamb√©m permanece est√°vel,  \n",
    "\n",
    "Ent√£o a s√©rie diferenciada tende a ser fracamente estacion√°ria (segunda ordem), o que √© essencial para aplicar modelos como ARIMA, SARIMA ou GARCH.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üî∂ ‚Çø -----> Diagn√≥stico Visual de Estacionariedade ‚Äî M√©dia e Desvio Padr√£o M√≥veis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "#  CONFIGURA√á√ïES DE PAR√ÇMETROS\n",
    "# =============================\n",
    "resample_freq = 'h'           # 'h' (hora), 'd' (dia), 'W' (semana)\n",
    "window_size = 30              # Tamanho da janela de m√©dia/desvio / Rolling window size\n",
    "date_range_start = None       # Ex: \"2023-01-01\"\n",
    "date_range_end = None         # Ex: \"2023-03-01\"\n",
    "\n",
    "# ===============\n",
    "#  PROCESSAMENTO\n",
    "# ===============\n",
    "\n",
    "df_Rolling = df_features_temp.toPandas()\n",
    "# --> Converte o DataFrame PySpark para Pandas / Converts PySpark DataFrame to Pandas <--\n",
    "\n",
    "df_Rolling[\"block_timestamp\"] = pd.to_datetime(df_Rolling[\"block_timestamp\"])\n",
    "# --> Garante que o campo de tempo seja do tipo datetime / Ensures timestamp is datetime type <--\n",
    "\n",
    "df_Rolling = df_Rolling.sort_values(\"block_timestamp\")\n",
    "# --> Ordena os dados por tempo crescente / Sorts data by time ascending <--\n",
    "\n",
    "df_Rolling = df_Rolling.set_index(\"block_timestamp\").resample(resample_freq).mean()\n",
    "# --> Reamostra os dados com base na frequ√™ncia definida (ex: 'h' para hora) / Resamples the data based on the defined frequency <--\n",
    "\n",
    "if date_range_start and date_range_end:\n",
    "    df_Rolling = df_Rolling.loc[date_range_start:date_range_end]\n",
    "# --> Filtra a s√©rie pelo intervalo de datas, se definido / Filters series by date range if provided <--\n",
    "\n",
    "df_Rolling[\"block_timestamp\"] = df_Rolling.index\n",
    "# --> Restaura a coluna de timestamp para uso em gr√°ficos / Restores timestamp column for plotting <--\n",
    "\n",
    "df_Rolling[\"btc_price_diff\"] = df_Rolling[\"btc_price_usd\"].diff()\n",
    "# --> Aplica diferencia√ß√£o de primeira ordem / Applies first-order differencing <--\n",
    "\n",
    "df_Rolling = df_Rolling.dropna()\n",
    "# --> Remove os valores nulos gerados pela diferencia√ß√£o / Drops NaN values caused by differencing <--\n",
    "\n",
    "# =================================\n",
    "#  C√ÅLCULO DE M√âDIA E DESVIO M√ìVEL\n",
    "# =================================\n",
    "\n",
    "rolling_mean = df_Rolling[\"btc_price_diff\"].rolling(window=window_size).mean()\n",
    "# --> Calcula a m√©dia m√≥vel com janela definida / Computes rolling mean with defined window <--\n",
    "\n",
    "rolling_std = df_Rolling[\"btc_price_diff\"].rolling(window=window_size).std()\n",
    "# --> Calcula o desvio padr√£o m√≥vel com janela definida / Computes rolling standard deviation <--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rolling_mean_std(df_Rolling, rolling_mean, rolling_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üî∂ ‚Çø -----> Lembrete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Teste KPSS ‚Äî Valida√ß√£o de Estacionariedade  \n",
    "\n",
    "O teste KPSS (Kwiatkowski‚ÄìPhillips‚ÄìSchmidt‚ÄìShin) avalia a hip√≥tese nula de que a s√©rie √© estacion√°ria (em n√≠vel ou tend√™ncia).  \n",
    "\n",
    "A s√©rie temporal $Z_t$ √© modelada como:  \n",
    "\n",
    "$\n",
    "Z_t = r_t + u_t  \n",
    "$\n",
    "\n",
    "Onde:\n",
    "$r_t$ √© uma tend√™ncia determin√≠stica (constante ou linear)\n",
    "$u_t$ √© um processo estacion√°rio\n",
    "\n",
    "‚∏ª\n",
    "\n",
    "Estat√≠stica do teste:\n",
    "\n",
    "$\n",
    "\\text{KPSS} = \\frac{1}{T^2} \\sum_{t=1}^{T} S_t^2 \\bigg/ \\hat{\\sigma}^2  \n",
    "$\n",
    "\n",
    "Com:\n",
    "$S_t = \\sum_{i=1}^{t} \\hat{u}_i$: soma acumulada dos res√≠duos  \n",
    "$\\hat{\\sigma}^2$: estimativa da vari√¢ncia dos res√≠duos via Newey-West  \n",
    "$T$: n√∫mero total de observa√ß√µes  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üî∂ ‚Çø -----> Teste KPSS ‚Äî Teste de Raiz Unit√°ria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================\n",
    "# TESTE DE ESTACIONARIEDADE (KPSS)\n",
    "# =================================\n",
    "\n",
    "# Suprimir apenas o InterpolationWarning\n",
    "from statsmodels.tools.sm_exceptions import InterpolationWarning\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", InterpolationWarning)\n",
    "    # --> Suprime o aviso de interpola√ß√£o gerado pelo KPSS / Suppresses interpolation warning from KPSS <--\n",
    "\n",
    "    stat, p_value, lags, crit = kpss(df_acf[\"btc_price_diff\"].dropna(), regression='c')\n",
    "    # --> Executa o teste KPSS na s√©rie diferenciada com tend√™ncia constante ('c') / Runs KPSS test with constant trend assumption <--\n",
    "\n",
    "print(\"KPSS Estat√≠stica:\", stat)\n",
    "# --> Estat√≠stica do teste KPSS (valores maiores indicam evid√™ncia contra estacionariedade) \n",
    "# --> KPSS test statistic (higher values indicate non-stationarity) <--\n",
    "\n",
    "print(\"p-valor:\", p_value)\n",
    "# --> p-valor do teste KPSS (p < 0.05 indica rejei√ß√£o da estacionariedade) / KPSS p-value (p < 0.05 suggests non-stationarity) <--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üî∂ ‚Çø -----> Lembrete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Fun√ß√£o de Autocorrela√ß√£o Parcial (PACF)  \n",
    "\n",
    "A Fun√ß√£o de Autocorrela√ß√£o Parcial (PACF) mede a correla√ß√£o entre os valores da s√©rie temporal com uma defasagem k, eliminando a influ√™ncia dos lags intermedi√°rios.  \n",
    "\n",
    "Ou seja, a PACF no lag k representa a correla√ß√£o entre Z(t) e Z(t-k) condicionada aos valores entre eles Z(t-1), Z(t-2), ‚Ä¶, Z(t-k+1).  \n",
    "\n",
    "Para uma s√©rie temporal Z(t), a PACF no lag k √© definida como o coeficiente \\phi_{kk} na regress√£o linear:  \n",
    "\n",
    "$\n",
    "Z(t) = \\phi_{k1}Z(t-1) + \\phi_{k2}Z(t-2) + \\cdots + \\phi_{kk}Z(t-k) + \\varepsilon_t  \n",
    "$\n",
    "\n",
    "Onde:  \n",
    "$\\phi_{kk}$ √© a autocorrela√ß√£o parcial no lag k   \n",
    "$\\varepsilon_t$ √© o erro branco (res√≠duo aleat√≥rio)  \n",
    "\n",
    "A PACF ajuda a identificar a ordem p de um modelo AR(p), pois mostra claramente at√© onde as correla√ß√µes s√£o significativas sem efeito indireto de lags intermedi√°rios.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üî∂ ‚Çø -----> Verifica√ß√£o de Autocorrela√ß√£o/Padr√µes com ACF x PACF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf_pacf(df_features_temp, resample_freq='h', nlags_option='serie_longa', manual_nlags=150, \n",
    "                  date_range_start=None, date_range_end=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# PAR√ÇMETROS GERAIS / GENERAL PARAMETERS\n",
    "# ================================================================\n",
    "window_size = 60     # --> Tamanho da janela deslizante / Sliding window size <--\n",
    "step = 10            # --> Passo da janela / Window step <--\n",
    "stationarity_results = []  # --> Lista para armazenar resultados / List to store results <--\n",
    "\n",
    "# ================================================================\n",
    "# LOOP DE EXTRA√á√ÉO / FEATURE EXTRACTION LOOP\n",
    "# ================================================================\n",
    "for i in range(0, len(df_fft) - window_size + 1, step):\n",
    "    # Subconjunto da s√©rie para an√°lise local\n",
    "    # / Local series slice for feature extraction\n",
    "    janela = df_fft[\"btc_price_usd\"].iloc[i:i + window_size]\n",
    "\n",
    "    # Dados de rastreio associados √† √∫ltima posi√ß√£o da janela\n",
    "    # / Metadata from the end of the window\n",
    "    block_height = df_fft[\"block_height\"].iloc[i + window_size - 1]\n",
    "    block_timestamp = df_fft[\"block_timestamp\"].iloc[i + window_size - 1]\n",
    "\n",
    "    # ================================\n",
    "    # SUPRESS√ÉO DE WARNINGS / WARNING SUPPRESSION\n",
    "    # ================================\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=InterpolationWarning)\n",
    "        warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "\n",
    "        try:\n",
    "            # ================================\n",
    "            # C√ÅLCULO DAS FEATURES DE ESTACIONARIEDADE\n",
    "            # STATIONARITY FEATURE EXTRACTION\n",
    "            # ================================\n",
    "            result = stationarity_diagnostics(\n",
    "                series=janela,\n",
    "                block_height=block_height,\n",
    "                block_timestamp=block_timestamp\n",
    "            )\n",
    "            # --> Executa fun√ß√£o de extra√ß√£o robusta para s√©ries curtas e longas / Calls robust function with short/long support <--\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[ERRO] Falha ao extrair m√©tricas na janela {i}:{i+window_size} ‚Äî {e}\")\n",
    "            # --> Em caso de falha, cria linha com NaNs / On failure, fallback to NaN <--\n",
    "            result = pd.DataFrame([{\n",
    "                \"block_height\": block_height,\n",
    "                \"block_timestamp\": block_timestamp,\n",
    "                \"adf_stat\": np.nan,\n",
    "                \"adf_pvalue\": np.nan,\n",
    "                \"kpss_stat\": np.nan,\n",
    "                \"kpss_pvalue\": np.nan\n",
    "            }])\n",
    "\n",
    "    stationarity_results.append(result)\n",
    "\n",
    "# ================================================================\n",
    "# CONSOLIDA√á√ÉO FINAL / FINAL CONSOLIDATION\n",
    "# ================================================================\n",
    "if stationarity_results:\n",
    "    stationarity = pd.concat(stationarity_results, ignore_index=True)\n",
    "    # --> Concatena todos os DataFrames extra√≠dos / Concatenates all result DataFrames <--\n",
    "    display(stationarity.head())\n",
    "else:\n",
    "    print(\"[AVISO] Nenhuma janela v√°lida para calcular estacionariedade.\")\n",
    "    stationarity = pd.DataFrame()\n",
    "    # --> Retorna DataFrame vazio se nada foi extra√≠do / Returns empty DataFrame if no extraction succeeded <--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Pr√©-Modelagem e Ajustes Estat√≠sticos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Modelos Lineares de Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üî∂ ‚Çø -----> Grid Search - ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suprime warnings de modelos que n√£o convergem\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# --> Suprime avisos durante o ajuste de modelos, especialmente de converg√™ncia / Suppresses warnings (e.g., convergence) during model fitting <--\n",
    "\n",
    "# ==============\n",
    "# CONFIGURA√á√ïES\n",
    "# ==============\n",
    "\n",
    "p_range = range(0, 4)  # Ex: 0 a 3\n",
    "d = 1                  # Grau de diferencia√ß√£o fixo\n",
    "q_range = range(0, 4)\n",
    "\n",
    "melhor_aic = float(\"inf\")\n",
    "# --> Inicializa o melhor AIC com infinito para garantir substitui√ß√£o / Initializes best AIC as infinity for comparison <--\n",
    "\n",
    "melhor_ordem = None\n",
    "melhor_modelo = None\n",
    "# --> Armazena a melhor ordem (p,d,q) e o melhor modelo ajustado / Stores best order and model instance <--\n",
    "\n",
    "# ====================\n",
    "# GRID SEARCH (p,d,q)\n",
    "# ====================\n",
    "\n",
    "for p in p_range:\n",
    "    for q in q_range:\n",
    "        try:\n",
    "            modelo = ARIMA(serie_diff, order=(p, d, q)).fit()\n",
    "            # --> Ajusta o modelo ARIMA para combina√ß√£o atual de (p,d,q) / Fits ARIMA model for current (p,d,q) combination <--\n",
    "\n",
    "            aic = modelo.aic\n",
    "            # --> Extrai o valor de AIC do modelo ajustado / Retrieves AIC from fitted model <--\n",
    "\n",
    "            if aic < melhor_aic:\n",
    "                melhor_aic = aic\n",
    "                melhor_ordem = (p, d, q)\n",
    "                melhor_modelo = modelo\n",
    "            # --> Atualiza se o AIC for o menor encontrado at√© agora / Updates best model if AIC is lower <--\n",
    "\n",
    "        except Exception:\n",
    "            continue\n",
    "        # --> Ignora erros em modelos que n√£o convergem / Skips models that raise errors <--\n",
    "\n",
    "# ==========\n",
    "# RESULTADO\n",
    "# ==========\n",
    "\n",
    "print(f\"Melhor ordem (p,d,q): {melhor_ordem}\")\n",
    "# --> Exibe a melhor combina√ß√£o de par√¢metros encontrada / Prints best order (p,d,q) found <--\n",
    "\n",
    "print(f\"Menor AIC: {melhor_aic:.2f}\")\n",
    "# --> Exibe o menor valor de AIC alcan√ßado / Prints lowest AIC achieved <--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üî∂ ‚Çø -----> Modelagem Matem√°tica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Modelagem Matem√°tica ‚Äî ARIMA (p, d, q)\n",
    "\n",
    "O modelo ARIMA (AutoRegressive Integrated Moving Average) √© uma combina√ß√£o de tr√™s componentes:  \n",
    "p: n√∫mero de termos autorregressivos (AR)  \n",
    "d: n√∫mero de diferencia√ß√µes necess√°rias para tornar a s√©rie estacion√°ria  \n",
    "q: n√∫mero de termos de m√©dia m√≥vel (MA)  \n",
    "\n",
    "‚∏ª\n",
    "\n",
    "A f√≥rmula geral do modelo ARIMA(p, d, q) √©:  \n",
    "\n",
    "$\n",
    "\\phi(B)(1 - B)^d Z_t = \\theta(B) \\varepsilon_t\n",
    "$\n",
    "\n",
    "Onde:  \n",
    "$Z_t$: valor da s√©rie temporal no tempo t  \n",
    "$B$: operador defasagem (lag), ou seja BZ_t = Z_{t-1}  \n",
    "$(1 - B)^d$: parte de diferencia√ß√£o (para tornar estacion√°ria)  \n",
    "$\\varepsilon_t$: termo de erro (ru√≠do branco)  \n",
    "$\\phi(B) = 1 - \\phi_1 B - \\phi_2 B^2 - \\dots - \\phi_p B^p$: parte AR  \n",
    "$\\theta(B) = 1 + \\theta_1 B + \\theta_2 B^2 + \\dots + \\theta_q B^q$: parte MA  \n",
    "\n",
    "‚∏ª\n",
    "\n",
    "Interpreta√ß√£o:  \n",
    "Se $d = 1$, estamos modelando a primeira diferen√ßa da s√©rie $ Y_t = Z_t - Z_{t-1}$.  \n",
    "O ARIMA ent√£o busca capturar padr√µes no comportamento das varia√ß√µes, n√£o dos valores absolutos.  \n",
    "Um bom modelo resulta em res√≠duos n√£o correlacionados, ou seja, ru√≠do branco.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üî∂ ‚Çø -----> ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, d, q = melhor_ordem  # Ex: (1, 1, 1)\n",
    "# --> Atribui os melhores par√¢metros encontrados pelo Grid Search / Assigns best (p,d,q) from Grid Search <--\n",
    "\n",
    "# Ajustar modelo ARIMA com a s√©rie diferenciada\n",
    "modelo = ARIMA(serie_diff, order=(p, d, q))\n",
    "# --> Inicializa o modelo ARIMA com a s√©rie diferenciada e a ordem selecionada / Initializes ARIMA model with differenced series and selected order <--\n",
    "\n",
    "modelo_ajustado = modelo.fit()\n",
    "# --> Ajusta o modelo aos dados / Fits the model to the data <--\n",
    "\n",
    "print(modelo_ajustado.summary())\n",
    "# --> Mostra o resumo estat√≠stico completo do modelo ARIMA ajustado / Displays full statistical summary of the fitted ARIMA model <--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üî∂ ‚Çø -----> Extra√ß√£o de Camadas Extra√≠das - ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_arima_layers(df_acf, p, d, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleciona uma janela da s√©rie de pre√ßos\n",
    "janela = df_fft[\"btc_price_usd\"].iloc[-60:]\n",
    "\n",
    "# Define as chaves de rastreio (√∫ltimo ponto da janela)\n",
    "block_height = df_fft[\"block_height\"].iloc[-1]\n",
    "block_timestamp = df_fft[\"block_timestamp\"].iloc[-1]\n",
    "\n",
    "# Executa a extra√ß√£o ARIMA com rastreabilidade\n",
    "df_arima_features = extract_arima_features(\n",
    "    series=janela,\n",
    "    block_height=block_height,\n",
    "    block_timestamp=block_timestamp,\n",
    "    order=(1, 1, 1)\n",
    ")\n",
    "\n",
    "# Visualiza o resultado\n",
    "display(df_arima_features.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regime = extract_regime_features(df_fft, col=\"btc_price_usd\", window=48)\n",
    "display(df_regime.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Diagnosticos de Residuos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üî∂ ‚Çø -----> ACF dos Res√≠duos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf_residuos(modelo_ajustado, nlags=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üî∂ ‚Çø -----> Teste de LJUNG-BOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# TESTE DE LJUNG-BOX PARA RES√çDUOS / LJUNG-BOX TEST FOR RESIDUALS\n",
    "# ================================================================\n",
    "def run_ljung_box_test(residuals, lags=10):\n",
    "    \"\"\"\n",
    "    Executa o teste de Ljung-Box com tratamento para s√©ries curtas.\n",
    "    / Runs Ljung-Box test with fallback for short residual series.\n",
    "\n",
    "    Par√¢metros / Parameters:\n",
    "    - residuals: pd.Series ou np.array\n",
    "        S√©rie de res√≠duos do modelo ajustado / Residual series from fitted model\n",
    "    - lags: int\n",
    "        N√∫mero de defasagens para o teste / Number of lags in the test\n",
    "\n",
    "    Retorna / Returns:\n",
    "    - dict com estat√≠stica e p-valor do teste\n",
    "    / dict with test statistic and p-value\n",
    "    \"\"\"\n",
    "    residuals = pd.Series(residuals).dropna()\n",
    "    # --> Remove valores ausentes dos res√≠duos / Drop NaNs from residuals <--\n",
    "\n",
    "    if len(residuals) <= lags:\n",
    "        print(f\"[AVISO] S√©rie de res√≠duos muito curta (n = {len(residuals)}) para {lags} lags. Retornando NaNs.\")\n",
    "        return {\"lb_stat\": np.nan, \"p_value\": np.nan}\n",
    "        # --> S√©rie insuficiente para o teste / Not enough residuals for the test <--\n",
    "\n",
    "    try:\n",
    "        ljung_result = acorr_ljungbox(residuals, lags=[lags], return_df=True)\n",
    "        return {\n",
    "            \"lb_stat\": ljung_result[\"lb_stat\"].iloc[0],\n",
    "            \"p_value\": ljung_result[\"lb_pvalue\"].iloc[0]\n",
    "        }\n",
    "        # --> Executa e retorna os resultados / Run test and return result <--\n",
    "    except Exception as e:\n",
    "        print(f\"[ERRO] Falha no teste de Ljung-Box: {e}\")\n",
    "        return {\"lb_stat\": np.nan, \"p_value\": np.nan}\n",
    "        # --> Em caso de falha, retorna NaNs / On failure, return NaNs <--\n",
    "\n",
    "# ================================================================\n",
    "# EXEMPLO DE APLICA√á√ÉO / APPLICATION EXAMPLE\n",
    "# ================================================================\n",
    "residuos = modelo_ajustado.resid.dropna()\n",
    "# --> Obt√©m os res√≠duos do modelo e remove valores nulos / Extracts residuals and drops NaN values <--\n",
    "\n",
    "ljung_result = run_ljung_box_test(residuos, lags=10)\n",
    "# --> Executa o teste de Ljung-Box com fallback para s√©ries curtas / Runs Ljung-Box with fallback <--\n",
    "\n",
    "print(\"Resultado do Teste de Ljung-Box:\")\n",
    "print(f\"Estat√≠stica: {ljung_result['lb_stat']}\")\n",
    "print(f\"p-valor: {ljung_result['p_value']}\")\n",
    "# --> Exibe estat√≠sticas e p-valor para autocorrela√ß√£o conjunta / Shows p-value for joint autocorrelation <--\n",
    "\n",
    "# ================================\n",
    "# INTERPRETA√á√ÉO / INTERPRETATION\n",
    "# ================================\n",
    "# Se p-valor > 0.05 ‚Üí res√≠duos n√£o t√™m autocorrela√ß√£o ‚Üí modelo √© adequado.\n",
    "# Se p-valor < 0.05 ‚Üí evid√™ncia de autocorrela√ß√£o nos res√≠duos ‚Üí modelo pode ser melhorado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üî∂ ‚Çø -----> Histograma dos res√≠duos + Curva Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_residuos_analysis(residuos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Consolida√ß√£o das Features Temporais"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
